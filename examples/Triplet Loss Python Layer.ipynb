{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caffe_root = '../'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "sys.path.insert(0, caffe_root + 'examples/tripletloss')\n",
    "import caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tripletloss/tripletloss_layer.py\n",
    "import caffe\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TripletLossLayer(caffe.Layer):\n",
    "    \"\"\"\n",
    "    Compute the Triplet Loss based on the Google's FaceNet paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self, bottom, top):\n",
    "        # check if input pair is a triplet\n",
    "        if len(bottom) != 3:\n",
    "            raise Exception(\"Need three inputs to compute triplet loss. The bottom length was {}\".format(len(bottom)))\n",
    "            \n",
    "        params = eval(self.param_str)\n",
    "        try:\n",
    "            self.margin = float(params['margin'])\n",
    "        except:\n",
    "            self.margin = 1.0\n",
    "\n",
    "    def reshape(self, bottom, top):\n",
    "        # check input shapes match\n",
    "        if bottom[0].count != bottom[1].count or bottom[1].count != bottom[2].count:\n",
    "            raise Exception(\"Inputs must have the same dimension.\")\n",
    "        # differences are shape of inputs\n",
    "        self.diff_pos = np.zeros_like(bottom[0].data, dtype=np.float32)\n",
    "        self.diff_neg = np.zeros_like(bottom[0].data, dtype=np.float32)\n",
    "        # normalize\n",
    "        self.norm_anc = self.normalize(bottom[0].data)\n",
    "        print('norm_anc = {}'.format(self.norm_anc))\n",
    "        self.norm_pos = self.normalize(bottom[1].data)\n",
    "        print('norm_pos = {}'.format(self.norm_pos))\n",
    "        self.norm_neg = self.normalize(bottom[2].data)\n",
    "        print('norm_neg = {}'.format(self.norm_neg))\n",
    "        # loss\n",
    "        self.batch_size = bottom[0].count / len(bottom)\n",
    "        print('batch_size = {}'.format(self.batch_size))\n",
    "        self.loss = np.zeros(self.batch_size, dtype=np.float32)\n",
    "        # loss output is scalar\n",
    "        top[0].reshape(1)\n",
    "        \n",
    "    def normalize(self, array):\n",
    "        # ||f(x)||_2=1\n",
    "        l2 = np.linalg.norm(array, ord=2, axis=1, keepdims=True)\n",
    "        # avoid to devide by zero\n",
    "        l2[l2==0] = 1\n",
    "        return array / l2\n",
    "\n",
    "    def forward(self, bottom, top):\n",
    "        \"\"\" computes a loss\n",
    "        Note that the Loss is not averaged by the number of triplet sets.\n",
    "        Loss = SUM[i->N](Di_pos - Di_neg + margin), 0 <= i <= N(the batch size)\n",
    "        Dpos = sqrt(L2(IMGi_acr - IMGi_pos))\n",
    "        Dneg = sqrt(L2(IMGi_acr - IMGi_neg))\n",
    "        \"\"\"\n",
    "        \n",
    "        self.diff_pos[...] = self.norm_anc - self.norm_pos\n",
    "        print('diff_pos = {}'.format(self.diff_pos))\n",
    "        self.diff_neg[...] = self.norm_anc - self.norm_neg\n",
    "        print('diff_neg = {}'.format(self.diff_neg))\n",
    "        dist_pos = np.sum(self.diff_pos**2, axis=1)\n",
    "        print('dist_pos = {}'.format(dist_pos))\n",
    "        dist_neg = np.sum(self.diff_neg**2, axis=1)\n",
    "        print('dist_neg = {}'.format(dist_neg))\n",
    "        # calculate a loss for each item\n",
    "        for i in range(self.batch_size):\n",
    "            loss = dist_pos[i] - dist_neg[i] + self.margin\n",
    "            print('loss[{}] = {}'.format(i, loss))\n",
    "            self.loss[i] = max(0, loss)\n",
    "        total_loss = np.sum(self.loss)\n",
    "        print('total loss = {}, mini_batch_size={}'.format(total_loss, self.batch_size))\n",
    "        top[0].data[...] = total_loss / self.batch_size\n",
    "\n",
    "    def backward(self, top, propagate_down, bottom):\n",
    "        \"\"\" computes a gradient w.r.t. each IMG\n",
    "        dL/dDorg = SUM[i->N]{2(IMGi_neg - IMGi_pos)} if Lossi > 0 else 0\n",
    "        dL/dDpos = SUM[i->N](-2(IMGi_anc - IMGi_pos)) if Lossi > 0 else 0\n",
    "        dL/dDneg = SUM[i->N](2(IMGi_anc - IMGi_neg)) if Lossi > 0 else 0\n",
    "        \"\"\"\n",
    "        # gradient w.r.t. Dorg\n",
    "        diff_org = self.norm_neg - self.norm_pos\n",
    "        for i in range(self.batch_size):\n",
    "            if self.loss[i] == 0:\n",
    "                diff_org[i] = 0\n",
    "        bottom[0].diff[...] = 2 * diff_org\n",
    "        print('org diff = {}'.format(bottom[0].diff))\n",
    "        \n",
    "        # gradient w.r.t. Dpos\n",
    "        for i in range(self.batch_size):\n",
    "            self.diff_pos[i] = 0\n",
    "        bottom[1].diff[...] = -2 * self.diff_pos\n",
    "        print('pos diff = {}'.format(bottom[1].diff))\n",
    "        \n",
    "        # gradient w.r.t. Dneg\n",
    "        for i in range(self.batch_size):\n",
    "            self.diff_neg[i] = 0\n",
    "        bottom[2].diff[...] = 2 * self.diff_neg\n",
    "        print('neg diff = {}'.format(bottom[2].diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello Test Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import numpy as np\n",
    "from caffe import layers as L\n",
    "\n",
    "def load_net(net_proto):\n",
    "    f = tempfile.NamedTemporaryFile(mode='w+', delete=False)\n",
    "    f.write(str(net_proto))\n",
    "    f.close()\n",
    "    return caffe.Net(f.name, caffe.TEST)\n",
    "\n",
    "def example_network(batch_size):\n",
    "    n = caffe.NetSpec()\n",
    "\n",
    "    # we use the dummy data layer to control the \n",
    "    # shape of the inputs to the layer we are testing\n",
    "    ip_dims = [3*batch_size, 3]\n",
    "    label_dims = [batch_size]\n",
    "    n.ip, n.label = L.DummyData(shape=[dict(dim=ip_dims),dict(dim=label_dims)],\n",
    "                                        transform_param=dict(scale=1.0/255.0),\n",
    "                                        ntop=2)\n",
    "    \n",
    "    n.slice_anc, n.slice_pos, n.slice_neg = L.Slice(n.ip, slice_param=dict(axis=0), ntop=3)\n",
    "    \n",
    "    n.triplet = L.Python(n.slice_anc, n.slice_pos, n.slice_neg, python_param=dict(module='tripletloss_layer', layer='TripletLossLayer', param_str='{\\\"margin\\\": 1.0}'))\n",
    "    return n.to_proto()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ANC = [1.0, 1.0, 1.0]\n",
    "# ||f(IMG_ANC)||_2 = sqrt(1**2 + 1**2 + 1**2) = 1.73...\n",
    "IMG_POS = [1.0, 1.0, 1.0]\n",
    "# ||f(IMG_POS)||_2 = sqrt(1**2 + 1**2 + 1**2) = 1.73...\n",
    "IMG_NEG = [0., 0., 0.]\n",
    "# ||f(IMG_NEG)||_2 = sqrt(0**2 + 0**2 + 0**2) = 0\n",
    "ip_data = np.array([IMG_ANC, IMG_POS, IMG_NEG], dtype=np.float)\n",
    "print('ip_data shape = {}'.format(ip_data.shape))\n",
    "\n",
    "net_proto = example_network(1)\n",
    "net = load_net(net_proto)\n",
    "net.blobs['ip'].data[...] = ip_data\n",
    "\n",
    "net.forward()\n",
    "\n",
    "for name in net.blobs:\n",
    "    print('{}'.format(name))\n",
    "    print('value = {}'.format(net.blobs[name].data))\n",
    "    \n",
    "net.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ANC = [1.0, 1.0, 1.0]\n",
    "IMG_POS = [0.5, 0.5, 0.5]\n",
    "IMG_NEG = [0., 0., 0.]\n",
    "ip_data = np.array([IMG_ANC, IMG_POS, IMG_NEG], dtype=np.float)\n",
    "\n",
    "net_proto = example_network(1)\n",
    "net = load_net(net_proto)\n",
    "net.blobs['ip'].data[...] = ip_data\n",
    "\n",
    "net.forward()\n",
    "\n",
    "for name in net.blobs:\n",
    "    print('{}'.format(name))\n",
    "    print('value = {}'.format(net.blobs[name].data))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ANC = [1.0, 1.0, 1.0]\n",
    "IMG_POS = [0., 0., 0.]\n",
    "IMG_NEG = [1.0, 1.0, 1.0]\n",
    "ip_data = np.array([IMG_ANC, IMG_POS, IMG_NEG], dtype=np.float)\n",
    "\n",
    "net_proto = example_network(1)\n",
    "net = load_net(net_proto)\n",
    "net.blobs['ip'].data[...] = ip_data\n",
    "\n",
    "net.forward()\n",
    "\n",
    "for name in net.blobs:\n",
    "    print('{}'.format(name))\n",
    "    print('value = {}'.format(net.blobs[name].data))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ANC = [1.0, 1.0, 1.0]\n",
    "IMG_POS = [0.5, 0.5, 0.5]\n",
    "IMG_NEG = [0.5, 0.5, 0.5]\n",
    "ip_data = np.array([IMG_ANC, IMG_POS, IMG_NEG], dtype=np.float)\n",
    "\n",
    "net_proto = example_network(1)\n",
    "net = load_net(net_proto)\n",
    "net.blobs['ip'].data[...] = ip_data\n",
    "\n",
    "net.forward()\n",
    "\n",
    "for name in net.blobs:\n",
    "    print('{}'.format(name))\n",
    "    print('value = {}'.format(net.blobs[name].data))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ANC = [1.0, 1.0, 1.0]\n",
    "IMG_ANC2 = [1.0, 1.0, 1.0]\n",
    "IMG_POS = [1.0, 1.0, 1.0]\n",
    "IMG_POS2 = [0.5, 0.5, 0.5]\n",
    "IMG_NEG = [0., 0., 0.]\n",
    "IMG_NEG2 = [0.5, 0.5, 0.5]\n",
    "ip_data = np.array([IMG_ANC, IMG_ANC2, IMG_POS, IMG_POS2, IMG_NEG, IMG_NEG2], dtype=np.float)\n",
    "\n",
    "net_proto = example_network(2)\n",
    "net = load_net(net_proto)\n",
    "net.blobs['ip'].data[...] = ip_data\n",
    "\n",
    "net.forward()\n",
    "\n",
    "for name in net.blobs:\n",
    "    print('{}'.format(name))\n",
    "    print('value = {}'.format(net.blobs[name].data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ANC = [10.0, 5.0, 100.0]\n",
    "IMG_POS = [30.0, 10.0, 20.0]\n",
    "IMG_NEG = [100., 2., 50.]\n",
    "ip_data = np.array([IMG_ANC, IMG_POS, IMG_NEG], dtype=np.float)\n",
    "print('ip_data shape = {}'.format(ip_data.shape))\n",
    "\n",
    "net_proto = example_network(1)\n",
    "net = load_net(net_proto)\n",
    "net.blobs['ip'].data[...] = ip_data\n",
    "\n",
    "net.forward()\n",
    "\n",
    "for name in net.blobs:\n",
    "    print('{}'.format(name))\n",
    "    print('value = {}'.format(net.blobs[name].data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello Training with MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tripletloss/mnist_tripletloss_train_test.prototxt\n",
    "name: \"mnist_tripletloss_train_test\"\n",
    "layer {\n",
    "  name: \"triplet_data\"\n",
    "  type: \"ImageData\"\n",
    "  top: \"triplet_data\"\n",
    "  top: \"label\"\n",
    "  include {\n",
    "    phase: TRAIN\n",
    "  }\n",
    "  transform_param {\n",
    "    scale: 0.00390625\n",
    "  }\n",
    "  image_data_param {\n",
    "    source: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/trainlist.txt\"\n",
    "    batch_size: 96\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"triplet_data\"\n",
    "  type: \"ImageData\"\n",
    "  top: \"triplet_data\"\n",
    "  top: \"label\"\n",
    "  include {\n",
    "    phase: TEST\n",
    "  }\n",
    "  transform_param {\n",
    "    scale: 0.00390625\n",
    "  }\n",
    "  image_data_param {\n",
    "    source: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/trainlist.txt\"\n",
    "    batch_size: 96\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"slice_triplet\"\n",
    "  type: \"Slice\"\n",
    "  bottom: \"triplet_data\"\n",
    "  top: \"anchor\"\n",
    "  top: \"positive\"\n",
    "  top: \"negative\"\n",
    "  slice_param {\n",
    "    slice_dim: 0\n",
    "  }\n",
    "}\n",
    "\n",
    "################# ANCHOR #############\n",
    "layer {\n",
    "  name: \"conv1\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"anchor\"\n",
    "  top: \"conv1\"\n",
    "  param {\n",
    "    name: \"conv1_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"conv1_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 20\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool1\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv1\"\n",
    "  top: \"pool1\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv2\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"pool1\"\n",
    "  top: \"conv2\"\n",
    "  param {\n",
    "    name: \"conv2_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"conv2_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 50\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool2\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv2\"\n",
    "  top: \"pool2\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"ip1\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"pool2\"\n",
    "  top: \"ip1\"\n",
    "  param {\n",
    "    name: \"ip1_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"ip1_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 500\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu1\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"ip1\"\n",
    "  top: \"ip1\"\n",
    "}\n",
    "layer {\n",
    "  name: \"ip2\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"ip1\"\n",
    "  top: \"ip2\"\n",
    "  param {\n",
    "    name: \"ip2_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"ip2_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 10\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"feat\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"ip2\"\n",
    "  top: \"feat\"\n",
    "  param {\n",
    "    name: \"feat_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"feat_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 2\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "###################### POSITIVE ###################\n",
    "\n",
    "layer {\n",
    "  name: \"conv1_p\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"positive\"\n",
    "  top: \"conv1_p\"\n",
    "  param {\n",
    "    name: \"conv1_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"conv1_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 20\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool1_p\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv1_p\"\n",
    "  top: \"pool1_p\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv2_p\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"pool1_p\"\n",
    "  top: \"conv2_p\"\n",
    "  param {\n",
    "    name: \"conv2_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"conv2_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 50\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool2_p\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv2_p\"\n",
    "  top: \"pool2_p\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"ip1_p\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"pool2_p\"\n",
    "  top: \"ip1_p\"\n",
    "  param {\n",
    "    name: \"ip1_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"ip1_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 500\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu1_p\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"ip1_p\"\n",
    "  top: \"ip1_p\"\n",
    "}\n",
    "layer {\n",
    "  name: \"ip2_p\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"ip1_p\"\n",
    "  top: \"ip2_p\"\n",
    "  param {\n",
    "    name: \"ip2_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"ip2_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 10\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"feat_p\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"ip2_p\"\n",
    "  top: \"feat_p\"\n",
    "  param {\n",
    "    name: \"feat_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"feat_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 2\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "######################### NEGATIVE #########################\n",
    "\n",
    "layer {\n",
    "  name: \"conv1_n\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"negative\"\n",
    "  top: \"conv1_n\"\n",
    "  param {\n",
    "    name: \"conv1_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"conv1_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 20\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool1_n\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv1_n\"\n",
    "  top: \"pool1_n\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv2_n\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"pool1_n\"\n",
    "  top: \"conv2_n\"\n",
    "  param {\n",
    "    name: \"conv2_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"conv2_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 50\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool2_n\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv2_n\"\n",
    "  top: \"pool2_n\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"ip1_n\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"pool2_n\"\n",
    "  top: \"ip1_n\"\n",
    "  param {\n",
    "    name: \"ip1_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"ip1_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 500\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu1_n\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"ip1_n\"\n",
    "  top: \"ip1_n\"\n",
    "}\n",
    "layer {\n",
    "  name: \"ip2_n\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"ip1_n\"\n",
    "  top: \"ip2_n\"\n",
    "  param {\n",
    "    name: \"ip2_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"ip2_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 10\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"feat_n\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"ip2_n\"\n",
    "  top: \"feat_n\"\n",
    "  param {\n",
    "    name: \"feat_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"feat_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 2\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "############# Triplet Loss ###############\n",
    "layer {\n",
    "  name: \"tripletloss\"\n",
    "  type: \"Python\"\n",
    "  bottom: \"feat\"\n",
    "  bottom: \"feat_p\"\n",
    "  bottom: \"feat_n\"\n",
    "  top: \"loss\"\n",
    "  python_param {\n",
    "    module: \"tripletloss_layer\"\n",
    "    layer: \"TripletLossLayer\"\n",
    "    param_str: '{\\\"margin\\\": 1.0}'\n",
    "  }\n",
    "  include{\n",
    "        phase: TRAIN\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import os\n",
    "mnist_data_dir = os.path.join(caffe_root, 'data/mnist')\n",
    "mndata = MNIST(mnist_data_dir)\n",
    "images, labels = mndata.load_training()\n",
    "print('loaded {} images, {} labels'.format(len(images), len(labels)))\n",
    "print('sample image at 0 = {}'.format(images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from StringIO import StringIO\n",
    "\n",
    "img_dir = '/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/images'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.makedirs(img_dir)\n",
    "\n",
    "# create a training list\n",
    "triplet_dict = {'anchor': None, 'positive': None, 'negative': None}\n",
    "anchor_list = StringIO()\n",
    "pos_list = StringIO()\n",
    "neg_list = StringIO()\n",
    "triplet_no = 0\n",
    "for i, l in zip(images, labels):\n",
    "    array = np.array(i)\n",
    "    img = array.reshape((28, 28))\n",
    "    \n",
    "    if triplet_dict['anchor'] is None:\n",
    "        # this becomes an anchor\n",
    "        triplet_dict['anchor'] = [img, l]\n",
    "    elif triplet_dict['positive'] is None:\n",
    "        # check if this is the same label\n",
    "        if triplet_dict['anchor'][1] == l:\n",
    "            # this becomes a postive one\n",
    "            triplet_dict['positive'] = [img, l]\n",
    "    elif triplet_dict['anchor'][1] != l:\n",
    "        # this becomes a negative one\n",
    "        triplet_dict['negative'] = [img, l]\n",
    "        \n",
    "    if triplet_dict['negative'] is None:\n",
    "        continue\n",
    "    \n",
    "    # write\n",
    "    anchor_path = os.path.join(img_dir, '{}_anchor.jpg'.format(triplet_no))\n",
    "    pos_path = os.path.join(img_dir, '{}_positive.jpg'.format(triplet_no))\n",
    "    neg_path = os.path.join(img_dir, '{}_negative.jpg'.format(triplet_no))\n",
    "    \n",
    "    # image\n",
    "    cv2.imwrite(anchor_path, triplet_dict['anchor'][0])\n",
    "    cv2.imwrite(pos_path, triplet_dict['positive'][0])\n",
    "    cv2.imwrite(neg_path, triplet_dict['negative'][0])\n",
    "    \n",
    "    # sample\n",
    "    anchor_list.write('{} {}\\n'.format(anchor_path, triplet_dict['anchor'][1]))\n",
    "    pos_list.write('{} {}\\n'.format(pos_path, triplet_dict['positive'][1]))\n",
    "    neg_list.write('{} {}\\n'.format(neg_path, triplet_dict['negative'][1]))\n",
    "    \n",
    "    # reset\n",
    "    triplet_dict['anchor'] = None\n",
    "    triplet_dict['positive'] = None\n",
    "    triplet_dict['negative'] = None\n",
    "    \n",
    "    triplet_no += 1\n",
    "    \n",
    "# finally, write sample list\n",
    "with open(os.path.join(img_dir, '../' ,'trainlist.txt'), 'w') as f:\n",
    "    # write anchors first\n",
    "    f.write(anchor_list.getvalue())\n",
    "    anchor_list.close()\n",
    "    # positive\n",
    "    f.write(pos_list.getvalue())\n",
    "    pos_list.close()\n",
    "    # negative\n",
    "    f.write(neg_list.getvalue())\n",
    "    neg_list.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tripletloss/mnist_tripletloss_solver.prototxt\n",
    "# The train/test net protocol buffer definition\n",
    "train_net: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist_tripletloss_train_test.prototxt\"\n",
    "test_net: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist_tripletloss_train_test.prototxt\"\n",
    "# test_iter specifies how many forward passes the test should carry out.\n",
    "# In the case of MNIST, we have test batch size 100 and 100 test iterations,\n",
    "# covering the full 10,000 testing images.\n",
    "test_iter: 100\n",
    "# Carry out testing every 500 training iterations.\n",
    "test_interval: 500\n",
    "# The base learning rate, momentum and the weight decay of the network.\n",
    "base_lr: 0.01\n",
    "momentum: 0.9\n",
    "weight_decay: 0.0005\n",
    "# The learning rate policy\n",
    "lr_policy: \"inv\"\n",
    "gamma: 0.0001\n",
    "power: 0.75\n",
    "# Display every 100 iterations\n",
    "display: 100\n",
    "# The maximum number of iterations\n",
    "max_iter: 10000\n",
    "# snapshot intermediate results\n",
    "snapshot: 5000\n",
    "snapshot_prefix: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "solver = caffe.SGDSolver('/home/researcher/caffe-tripletloss/examples/tripletloss/mnist_tripletloss_solver.prototxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each output is (batch size, feature dim, spatial dim)\n",
    "[(k, v.data.shape) for k, v in solver.net.blobs.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just print the weight sizes (we'll omit the biases)\n",
    "[(k, v[0].data.shape) for k, v in solver.net.params.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.net.forward()  # train net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "niter = 10\n",
    "# losses will also be stored in the log\n",
    "train_loss = np.zeros(niter)\n",
    "\n",
    "# the main solver loop\n",
    "for it in range(niter):\n",
    "    print('~~~~~~~~~~ iteration {} ~~~~~~~~~~~~'.format(it))\n",
    "    solver.step(1)  # SGD by Caffe\n",
    "    \n",
    "    # store the train loss\n",
    "    train_loss[it] = solver.net.blobs['loss'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenCV3.4.2+NVCaffeWithTriplet",
   "language": "python",
   "name": "py2_caffe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
