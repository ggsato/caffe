{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caffe_root = '../'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "sys.path.insert(0, caffe_root + 'examples/tripletloss')\n",
    "import caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello Test Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import numpy as np\n",
    "from caffe import layers as L, params as P\n",
    "\n",
    "def load_net(net_proto):\n",
    "    f = tempfile.NamedTemporaryFile(mode='w+', delete=False)\n",
    "    f.write(str(net_proto))\n",
    "    f.close()\n",
    "    return caffe.Net(f.name, caffe.TEST)\n",
    "\n",
    "def l2normed(embeddings, dim):\n",
    "    \"\"\"Returns L2-normalized instances of vec; i.e., for each instance x in embeddings,\n",
    "    computes  x / ((x ** 2).sum() ** 0.5). Assumes embeddings has shape N x dim.\"\"\"\n",
    "    denom = L.Reduction(embeddings, axis=1, operation=P.Reduction.SUMSQ)\n",
    "    denom = L.Power(denom, power=(-0.5))\n",
    "    denom = L.Reshape(denom, num_axes=0, axis=-1, shape=dict(dim=[1]))\n",
    "    denom = L.Tile(denom, axis=1, tiles=dim)\n",
    "    return L.Eltwise(embeddings, denom, operation=P.Eltwise.PROD)\n",
    "\n",
    "def example_network(batch_size):\n",
    "    n = caffe.NetSpec()\n",
    "\n",
    "    # we use the dummy data layer to control the \n",
    "    # shape of the inputs to the layer we are testing\n",
    "    ip_dims = [3*batch_size, 3]\n",
    "    label_dims = [batch_size]\n",
    "    n.ip, n.label = L.DummyData(shape=[dict(dim=ip_dims),dict(dim=label_dims)],\n",
    "                                        transform_param=dict(scale=1.0/255.0),\n",
    "                                        ntop=2)\n",
    "    \n",
    "    n.slice_anc, n.slice_pos, n.slice_neg = L.Slice(n.ip, slice_param=dict(axis=0), ntop=3)\n",
    "    n.slice_anc_norm = l2normed(n.slice_anc, 3)\n",
    "    n.slice_pos_norm = l2normed(n.slice_pos, 3)\n",
    "    n.slice_neg_norm = l2normed(n.slice_neg, 3)\n",
    "    n.triplet = L.Python(n.slice_anc_norm, n.slice_pos_norm, n.slice_neg_norm, loss_weight=1, python_param=dict(module='tripletloss_layer', layer='TripletLossLayer', param_str='{\\\"margin\\\": 1.0, \\\"debug\\\": 1}'))\n",
    "    return n.to_proto()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ANC = [1.0, 1.0, 1.0]\n",
    "# ||f(IMG_ANC)||_2 = sqrt(1**2 + 1**2 + 1**2) = 1.73...\n",
    "IMG_POS = [1.0, 1.0, 1.0]\n",
    "# ||f(IMG_POS)||_2 = sqrt(1**2 + 1**2 + 1**2) = 1.73...\n",
    "IMG_NEG = [0., 0., 0.]\n",
    "# ||f(IMG_NEG)||_2 = sqrt(0**2 + 0**2 + 0**2) = 0\n",
    "\n",
    "# embeddings is an 1D-array of features\n",
    "# here, the size of features is 3, 3*32bit = 96bit\n",
    "# (batch_size, feature_size)\n",
    "embeddings = np.array([IMG_ANC, IMG_POS, IMG_NEG], dtype=np.float32)\n",
    "print('embeddings shape = {}'.format(embeddings.shape))\n",
    "\n",
    "net_proto = example_network(1)\n",
    "with open('tripletloss/mnist_tripletloss_train_test_10_auto.prototxt', 'w') as f:\n",
    "    f.write(str(net_proto))\n",
    "net = load_net(net_proto)\n",
    "net.blobs['ip'].data[...] = embeddings\n",
    "\n",
    "net.forward()\n",
    "\n",
    "for name in net.blobs:\n",
    "    print('{}'.format(name))\n",
    "    print('value = {}'.format(net.blobs[name].data))\n",
    "    \n",
    "print('running backward...')\n",
    "net.backward()\n",
    "\n",
    "print('diff anc = {}'.format(net.blobs['slice_anc'].diff))\n",
    "print('diff pos = {}'.format(net.blobs['slice_pos'].diff))\n",
    "print('diff neg = {}'.format(net.blobs['slice_neg'].diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ANC = [10.0, 5.0, 100.0]\n",
    "IMG_POS = [30.0, 10.0, 20.0]\n",
    "IMG_NEG = [100., 2., 50.]\n",
    "embeddings = np.array([IMG_ANC, IMG_POS, IMG_NEG], dtype=np.float32)\n",
    "print('embeddings shape = {}'.format(embeddings.shape))\n",
    "\n",
    "net_proto = example_network(1)\n",
    "net = load_net(net_proto)\n",
    "net.blobs['ip'].data[...] = embeddings\n",
    "\n",
    "net.forward()\n",
    "\n",
    "for name in net.blobs:\n",
    "    print('{}'.format(name))\n",
    "    print('value = {}'.format(net.blobs[name].data))\n",
    "    \n",
    "print('running backward...')\n",
    "net.backward()\n",
    "\n",
    "print('diff anc = {}'.format(net.blobs['slice_anc'].diff))\n",
    "print('diff pos = {}'.format(net.blobs['slice_pos'].diff))\n",
    "print('diff neg = {}'.format(net.blobs['slice_neg'].diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# omoindrot's Training with MNIST\n",
    "\n",
    "embeddings size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tripletloss/mnist_omoindrot_tripletloss_train_test_10.prototxt\n",
    "name: \"mnist_tripletloss_train_test_10\"\n",
    "layer {\n",
    "  name: \"triplet_data\"\n",
    "  type: \"ImageData\"\n",
    "  top: \"triplet_data\"\n",
    "  top: \"label\"\n",
    "  include {\n",
    "    phase: TRAIN\n",
    "  }\n",
    "  transform_param {\n",
    "    scale: 0.00390625\n",
    "  }\n",
    "  image_data_param {\n",
    "    source: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/trainlist_64.txt\"\n",
    "    batch_size: 192\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"triplet_data\"\n",
    "  type: \"ImageData\"\n",
    "  top: \"triplet_data\"\n",
    "  top: \"label\"\n",
    "  include {\n",
    "    phase: TEST\n",
    "  }\n",
    "  transform_param {\n",
    "    scale: 0.00390625\n",
    "  }\n",
    "  image_data_param {\n",
    "    source: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/trainlist_64.txt\"\n",
    "    batch_size: 192\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"slice_triplet\"\n",
    "  type: \"Slice\"\n",
    "  bottom: \"triplet_data\"\n",
    "  top: \"anchor\"\n",
    "  top: \"positive\"\n",
    "  top: \"negative\"\n",
    "  slice_param {\n",
    "    slice_dim: 0\n",
    "  }\n",
    "}\n",
    "\n",
    "################# ANCHOR #############\n",
    "layer {\n",
    "  name: \"conv1\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"anchor\"\n",
    "  top: \"conv1\"\n",
    "  param {\n",
    "    name: \"conv1_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"conv1_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 20\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool1\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv1\"\n",
    "  top: \"pool1\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv2\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"pool1\"\n",
    "  top: \"conv2\"\n",
    "  param {\n",
    "    name: \"conv2_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"conv2_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 50\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool2\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv2\"\n",
    "  top: \"pool2\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"ip1\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"pool2\"\n",
    "  top: \"ip1\"\n",
    "  param {\n",
    "    name: \"ip1_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"ip1_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 500\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu1\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"ip1\"\n",
    "  top: \"ip1\"\n",
    "}\n",
    "layer {\n",
    "  name: \"ip2\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"ip1\"\n",
    "  top: \"feat\"\n",
    "  param {\n",
    "    name: \"ip2_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"ip2_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 10\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "############# L2 Normalization ############\n",
    "\n",
    "layer {\n",
    "  name: \"Reduction1\"\n",
    "  type: \"Reduction\"\n",
    "  bottom: \"feat\"\n",
    "  top: \"Reduction1\"\n",
    "  reduction_param {\n",
    "    operation: SUMSQ\n",
    "    axis: 1\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"Power1\"\n",
    "  type: \"Power\"\n",
    "  bottom: \"Reduction1\"\n",
    "  top: \"Power1\"\n",
    "  power_param {\n",
    "    power: -0.5\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"Reshape1\"\n",
    "  type: \"Reshape\"\n",
    "  bottom: \"Power1\"\n",
    "  top: \"Reshape1\"\n",
    "  reshape_param {\n",
    "    shape {\n",
    "      dim: 1\n",
    "    }\n",
    "    axis: -1\n",
    "    num_axes: 0\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"Tile1\"\n",
    "  type: \"Tile\"\n",
    "  bottom: \"Reshape1\"\n",
    "  top: \"Tile1\"\n",
    "  tile_param {\n",
    "    axis: 1\n",
    "    tiles: 10\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"slice_anc_norm\"\n",
    "  type: \"Eltwise\"\n",
    "  bottom: \"feat\"\n",
    "  bottom: \"Tile1\"\n",
    "  top: \"slice_anc_norm\"\n",
    "  eltwise_param {\n",
    "    operation: PROD\n",
    "  }\n",
    "}\n",
    "\n",
    "############# Triplet Loss ###############\n",
    "layer {\n",
    "  name: \"tripletloss\"\n",
    "  type: \"Python\"\n",
    "  bottom: \"slice_anc_norm\"\n",
    "  top: \"loss\"\n",
    "  loss_weight: 1\n",
    "  python_param {\n",
    "    module: \"tripletloss_layer\"\n",
    "    layer: \"TripletLossLayer\"\n",
    "    param_str: '{\\\"margin\\\": 1.0}'\n",
    "  }\n",
    "  include {\n",
    "    phase: TRAIN\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pos_dist\"\n",
    "  type: \"Python\"\n",
    "  bottom: \"slice_anc_norm\"\n",
    "  bottom: \"slice_pos_norm\"\n",
    "  top: \"pos_dist\"\n",
    "  python_param {\n",
    "    module: \"tripletloss_layer\"\n",
    "    layer: \"PairwiseDistanceLayer\"\n",
    "    param_str: '{\\\"debug\\\": 0}'\n",
    "  }\n",
    "  include {\n",
    "    phase: TEST\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"neg_dist\"\n",
    "  type: \"Python\"\n",
    "  bottom: \"slice_anc_norm\"\n",
    "  bottom: \"slice_neg_norm\"\n",
    "  top: \"neg_dist\"\n",
    "  python_param {\n",
    "    module: \"tripletloss_layer\"\n",
    "    layer: \"PairwiseDistanceLayer\"\n",
    "    param_str: '{\\\"debug\\\": 0}'\n",
    "  }\n",
    "  include {\n",
    "    phase: TEST\n",
    "  }\n",
    "}\n",
    "layer{\n",
    "  name: \"silence\"\n",
    "  type: \"Silence\"\n",
    "  bottom: \"label\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import os\n",
    "mnist_data_dir = os.path.join(caffe_root, 'data/mnist')\n",
    "mndata = MNIST(mnist_data_dir)\n",
    "images, labels = mndata.load_training()\n",
    "print('loaded {} images, {} labels'.format(len(images), len(labels)))\n",
    "print('sample image at 0 = {}'.format(images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from StringIO import StringIO\n",
    "\n",
    "img_dir = '/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/images'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.makedirs(img_dir)\n",
    "\n",
    "# create a training list\n",
    "triplet_dict = {'anchor': None, 'positive': None, 'negative': None}\n",
    "anchor_list = StringIO()\n",
    "pos_list = StringIO()\n",
    "neg_list = StringIO()\n",
    "batch_str = StringIO()\n",
    "triplet_no = 0\n",
    "batch_size = 0\n",
    "for i, l in zip(images, labels):\n",
    "    array = np.array(i)\n",
    "    img = array.reshape((28, 28))\n",
    "    \n",
    "    if triplet_dict['anchor'] is None:\n",
    "        # this becomes an anchor\n",
    "        triplet_dict['anchor'] = [img, l]\n",
    "    elif triplet_dict['positive'] is None:\n",
    "        # check if this is the same label\n",
    "        if triplet_dict['anchor'][1] == l:\n",
    "            # this becomes a postive one\n",
    "            triplet_dict['positive'] = [img, l]\n",
    "    elif triplet_dict['anchor'][1] != l:\n",
    "        # this becomes a negative one\n",
    "        triplet_dict['negative'] = [img, l]\n",
    "        \n",
    "    if triplet_dict['negative'] is None:\n",
    "        continue\n",
    "    \n",
    "    # write\n",
    "    anchor_path = os.path.join(img_dir, '{}_anchor.jpg'.format(triplet_no))\n",
    "    pos_path = os.path.join(img_dir, '{}_positive.jpg'.format(triplet_no))\n",
    "    neg_path = os.path.join(img_dir, '{}_negative.jpg'.format(triplet_no))\n",
    "    \n",
    "    # image\n",
    "    cv2.imwrite(anchor_path, triplet_dict['anchor'][0])\n",
    "    cv2.imwrite(pos_path, triplet_dict['positive'][0])\n",
    "    cv2.imwrite(neg_path, triplet_dict['negative'][0])\n",
    "    \n",
    "    # sample\n",
    "    anchor_list.write('{} {}\\n'.format(anchor_path, triplet_dict['anchor'][1]))\n",
    "    pos_list.write('{} {}\\n'.format(pos_path, triplet_dict['positive'][1]))\n",
    "    neg_list.write('{} {}\\n'.format(neg_path, triplet_dict['negative'][1]))\n",
    "    \n",
    "    # reset\n",
    "    triplet_dict['anchor'] = None\n",
    "    triplet_dict['positive'] = None\n",
    "    triplet_dict['negative'] = None\n",
    "    \n",
    "    triplet_no += 1\n",
    "    batch_size += 1\n",
    "    \n",
    "    if batch_size == 64:\n",
    "        # write anchors first\n",
    "        batch_str.write(anchor_list.getvalue())\n",
    "        anchor_list.close()\n",
    "        anchor_list = StringIO()\n",
    "        # positive\n",
    "        batch_str.write(pos_list.getvalue())\n",
    "        pos_list.close()\n",
    "        pos_list = StringIO()\n",
    "        # negative\n",
    "        batch_str.write(neg_list.getvalue())\n",
    "        neg_list.close()\n",
    "        neg_list = StringIO()\n",
    "        # reset\n",
    "        batch_size = 0\n",
    "    \n",
    "# finally, write sample list\n",
    "with open(os.path.join(img_dir, '../' ,'trainlist_64.txt'), 'w') as f:\n",
    "    f.write(batch_str.getvalue())\n",
    "    batch_str.close()\n",
    "    anchor_list.close()\n",
    "    pos_list.close()\n",
    "    neg_list.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tripletloss/mnist_tripletloss_solver_10.prototxt\n",
    "# The train/test net protocol buffer definition\n",
    "train_net: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist_tripletloss_train_test_10.prototxt\"\n",
    "test_net: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist_tripletloss_train_test_10.prototxt\"\n",
    "# samples = 192 * 77 = 14784\n",
    "test_iter: 77\n",
    "# test at every epoch\n",
    "test_interval: 77\n",
    "# The base learning rate, momentum and the weight decay of the network.\n",
    "base_lr: 0.01\n",
    "momentum: 0.9\n",
    "weight_decay: 0.0005\n",
    "# The learning rate policy\n",
    "lr_policy: \"inv\"\n",
    "gamma: 0.0001\n",
    "power: 0.75\n",
    "# Display every epoch\n",
    "display: 77\n",
    "# The maximum number of iterations = 10 epochs\n",
    "max_iter: 770\n",
    "# snapshot intermediate results at every epoch\n",
    "snapshot: 77\n",
    "snapshot_prefix: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/mnist_tripletloss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "# reset solver to avoid a continuous training over multiple runs\n",
    "solver = None\n",
    "solver = caffe.SGDSolver('/home/researcher/caffe-tripletloss/examples/tripletloss/mnist_tripletloss_solver_10.prototxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each output is (batch size, feature dim, spatial dim)\n",
    "[(k, v.data.shape) for k, v in solver.net.blobs.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just print the weight sizes (we'll omit the biases)\n",
    "[(k, v[0].data.shape) for k, v in solver.net.params.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "itr_per_epoch = 77\n",
    "niter = itr_per_epoch * 20\n",
    "\n",
    "train_loss = np.zeros(niter)\n",
    "\n",
    "# the main solver loop\n",
    "for it in range(niter):\n",
    "    solver.step(1)  # SGD by Caffe\n",
    "    \n",
    "    # store the train loss\n",
    "    loss = solver.net.blobs['loss'].data\n",
    "    \n",
    "    # output every epoch\n",
    "    if it % itr_per_epoch == 0:\n",
    "        print('loss at epoch {} = {}'.format(it/itr_per_epoch, loss))\n",
    "    \n",
    "    train_loss[it] = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deploy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tripletloss/mnist_tripletloss_deploy_10.prototxt\n",
    "name: \"mnist_tripletloss_deploy_10\"\n",
    "layer {\n",
    "  name: \"data\"\n",
    "  type: \"Input\"\n",
    "  top: \"data\"\n",
    "  input_param { shape: { dim: 2 dim: 3 dim: 28 dim: 28 } }\n",
    "}\n",
    "layer {\n",
    "  name: \"slice_pair\"\n",
    "  type: \"Slice\"\n",
    "  bottom: \"data\"\n",
    "  top: \"foo\"\n",
    "  top: \"bar\"\n",
    "  slice_param {\n",
    "    slice_dim: 0\n",
    "  }\n",
    "}\n",
    "\n",
    "# foo => anchor\n",
    "# bar => positive\n",
    "\n",
    "################# ANCHOR #############\n",
    "layer {\n",
    "  name: \"conv1\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"foo\"\n",
    "  top: \"conv1\"\n",
    "  param {\n",
    "    name: \"conv1_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"conv1_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 20\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool1\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv1\"\n",
    "  top: \"pool1\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv2\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"pool1\"\n",
    "  top: \"conv2\"\n",
    "  param {\n",
    "    name: \"conv2_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"conv2_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 50\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool2\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv2\"\n",
    "  top: \"pool2\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"ip1\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"pool2\"\n",
    "  top: \"ip1\"\n",
    "  param {\n",
    "    name: \"ip1_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"ip1_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 500\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu1\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"ip1\"\n",
    "  top: \"ip1\"\n",
    "}\n",
    "layer {\n",
    "  name: \"ip2\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"ip1\"\n",
    "  top: \"feat\"\n",
    "  param {\n",
    "    name: \"ip2_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"ip2_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 10\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "###################### POSITIVE ###################\n",
    "\n",
    "layer {\n",
    "  name: \"conv1_p\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"bar\"\n",
    "  top: \"conv1_p\"\n",
    "  param {\n",
    "    name: \"conv1_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"conv1_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 20\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool1_p\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv1_p\"\n",
    "  top: \"pool1_p\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv2_p\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"pool1_p\"\n",
    "  top: \"conv2_p\"\n",
    "  param {\n",
    "    name: \"conv2_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"conv2_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 50\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool2_p\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv2_p\"\n",
    "  top: \"pool2_p\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"ip1_p\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"pool2_p\"\n",
    "  top: \"ip1_p\"\n",
    "  param {\n",
    "    name: \"ip1_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"ip1_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 500\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu1_p\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"ip1_p\"\n",
    "  top: \"ip1_p\"\n",
    "}\n",
    "layer {\n",
    "  name: \"ip2_p\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"ip1_p\"\n",
    "  top: \"feat_p\"\n",
    "  param {\n",
    "    name: \"ip2_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"ip2_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 10\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "############# L2 Normalization ############\n",
    "\n",
    "layer {\n",
    "  name: \"Reduction1\"\n",
    "  type: \"Reduction\"\n",
    "  bottom: \"feat\"\n",
    "  top: \"Reduction1\"\n",
    "  reduction_param {\n",
    "    operation: SUMSQ\n",
    "    axis: 1\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"Power1\"\n",
    "  type: \"Power\"\n",
    "  bottom: \"Reduction1\"\n",
    "  top: \"Power1\"\n",
    "  power_param {\n",
    "    power: -0.5\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"Reshape1\"\n",
    "  type: \"Reshape\"\n",
    "  bottom: \"Power1\"\n",
    "  top: \"Reshape1\"\n",
    "  reshape_param {\n",
    "    shape {\n",
    "      dim: 1\n",
    "    }\n",
    "    axis: -1\n",
    "    num_axes: 0\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"Tile1\"\n",
    "  type: \"Tile\"\n",
    "  bottom: \"Reshape1\"\n",
    "  top: \"Tile1\"\n",
    "  tile_param {\n",
    "    axis: 1\n",
    "    tiles: 10\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"slice_anc_norm\"\n",
    "  type: \"Eltwise\"\n",
    "  bottom: \"feat\"\n",
    "  bottom: \"Tile1\"\n",
    "  top: \"slice_anc_norm\"\n",
    "  eltwise_param {\n",
    "    operation: PROD\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"Reduction2\"\n",
    "  type: \"Reduction\"\n",
    "  bottom: \"feat_p\"\n",
    "  top: \"Reduction2\"\n",
    "  reduction_param {\n",
    "    operation: SUMSQ\n",
    "    axis: 1\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"Power2\"\n",
    "  type: \"Power\"\n",
    "  bottom: \"Reduction2\"\n",
    "  top: \"Power2\"\n",
    "  power_param {\n",
    "    power: -0.5\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"Reshape2\"\n",
    "  type: \"Reshape\"\n",
    "  bottom: \"Power2\"\n",
    "  top: \"Reshape2\"\n",
    "  reshape_param {\n",
    "    shape {\n",
    "      dim: 1\n",
    "    }\n",
    "    axis: -1\n",
    "    num_axes: 0\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"Tile2\"\n",
    "  type: \"Tile\"\n",
    "  bottom: \"Reshape2\"\n",
    "  top: \"Tile2\"\n",
    "  tile_param {\n",
    "    axis: 1\n",
    "    tiles: 10\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"slice_pos_norm\"\n",
    "  type: \"Eltwise\"\n",
    "  bottom: \"feat_p\"\n",
    "  bottom: \"Tile2\"\n",
    "  top: \"slice_pos_norm\"\n",
    "  eltwise_param {\n",
    "    operation: PROD\n",
    "  }\n",
    "}\n",
    "\n",
    "############# Triplet Loss ###############\n",
    "layer {\n",
    "  name: \"pos_dist\"\n",
    "  type: \"Python\"\n",
    "  bottom: \"slice_anc_norm\"\n",
    "  bottom: \"slice_pos_norm\"\n",
    "  top: \"pos_dist\"\n",
    "  python_param {\n",
    "    module: \"tripletloss_layer\"\n",
    "    layer: \"PairwiseDistanceLayer\"\n",
    "    param_str: '{\\\"debug\\\": 0}'\n",
    "  }\n",
    "  include {\n",
    "    phase: TEST\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# display plots in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# set display defaults\n",
    "plt.rcParams['figure.figsize'] = (10, 10)        # large images\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
    "plt.rcParams['image.cmap'] = 'gray'  # use grayscale output rather than a (potentially misleading) color heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_def = caffe_root + 'examples/tripletloss/mnist_tripletloss_deploy_10.prototxt'\n",
    "model_weights = caffe_root + 'examples/tripletloss/mnist/mnist_tripletloss_iter_1463.caffemodel'\n",
    "\n",
    "net = caffe.Net(model_def,      # defines the structure of the model\n",
    "                model_weights,  # contains the trained weights\n",
    "                caffe.TEST)     # use test mode (e.g., don't perform dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_data\n",
    "\n",
    "mnistテストセット一覧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls /home/researcher/caffe-tripletloss/examples/tripletloss/mnist/images/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img_anc = cv2.imread('/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/images/0_anchor.jpg')\n",
    "print('img_anc shape = {}'.format(img_anc.shape))\n",
    "plt.imshow(img_anc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pos = cv2.imread('/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/images/0_positive.jpg')\n",
    "print('img_pos shape = {}'.format(img_pos.shape))\n",
    "plt.imshow(img_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_neg = cv2.imread('/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/images/0_negative.jpg')\n",
    "print('img_neg shape = {}'.format(img_neg.shape))\n",
    "plt.imshow(img_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_anc_transposed = img_anc.transpose(2, 0, 1)\n",
    "print('img_anc_transposed shape = {}'.format(img_anc_transposed.shape))\n",
    "img_pos_transposed = img_pos.transpose(2, 0, 1)\n",
    "print('img_pos_transposed shape = {}'.format(img_pos_transposed.shape))\n",
    "img_neg_transposed = img_neg.transpose(2, 0, 1)\n",
    "print('img_neg_transposed shape = {}'.format(img_neg_transposed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a pair data\n",
    "net.blobs['data'].data[...] = np.array([img_anc_transposed, img_pos_transposed])\n",
    "\n",
    "# calculate distance\n",
    "net.forward()\n",
    "\n",
    "dist = net.blobs['pos_dist'].data\n",
    "print('dist between anc and pos = {}'.format(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a pair data\n",
    "net.blobs['data'].data[...] = np.array([img_anc_transposed, img_neg_transposed])\n",
    "\n",
    "# calculate distance\n",
    "net.forward()\n",
    "\n",
    "dist = net.blobs['pos_dist'].data\n",
    "print('dist between anc and neg = {}'.format(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test no.1の時"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_anc = cv2.imread('/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/images/1_anchor.jpg')\n",
    "print('img_anc shape = {}'.format(img_anc.shape))\n",
    "plt.imshow(img_anc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pos = cv2.imread('/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/images/1_positive.jpg')\n",
    "print('img_anc shape = {}'.format(img_anc.shape))\n",
    "plt.imshow(img_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_neg = cv2.imread('/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/images/1_negative.jpg')\n",
    "print('img_anc shape = {}'.format(img_anc.shape))\n",
    "plt.imshow(img_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_anc_transposed = img_anc.transpose(2, 0, 1)\n",
    "img_pos_transposed = img_pos.transpose(2, 0, 1)\n",
    "img_neg_transposed = img_neg.transpose(2, 0, 1)\n",
    "\n",
    "# set a pair data\n",
    "net.blobs['data'].data[...] = np.array([img_anc_transposed, img_pos_transposed])\n",
    "\n",
    "# calculate distance\n",
    "net.forward()\n",
    "\n",
    "dist = net.blobs['pos_dist'].data\n",
    "print('dist between anc and pos = {}'.format(dist))\n",
    "\n",
    "# set a pair data\n",
    "net.blobs['data'].data[...] = np.array([img_anc_transposed, img_neg_transposed])\n",
    "\n",
    "# calculate distance\n",
    "net.forward()\n",
    "\n",
    "dist = net.blobs['pos_dist'].data\n",
    "print('dist between anc and neg = {}'.format(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test No. n で実行してみる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change 'test_num', then you will get images and the distances of the coressponding test set number.\n",
    "test_num = 50\n",
    "\n",
    "MNIST_ROOT = \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/images/\"\n",
    "ANC_PATH = MNIST_ROOT + str(test_num) + \"_anchor.jpg\"\n",
    "POS_PATH = MNIST_ROOT + str(test_num) + \"_positive.jpg\"\n",
    "NEG_PATH = MNIST_ROOT + str(test_num) + \"_negative.jpg\"\n",
    "\n",
    "#\n",
    "img_anc = cv2.imread(ANC_PATH)\n",
    "print('img_anc shape = {}'.format(img_anc.shape))\n",
    "\n",
    "img_pos = cv2.imread(POS_PATH)\n",
    "print('img_anc shape = {}'.format(img_anc.shape))\n",
    "\n",
    "img_neg = cv2.imread(NEG_PATH)\n",
    "print('img_anc shape = {}'.format(img_anc.shape))\n",
    "\n",
    "img_verticle = np.concatenate((img_anc, img_pos, img_neg), axis = 0) #縦\n",
    "plt.imshow(img_verticle)\n",
    "\n",
    "img_anc_transposed = img_anc.transpose(2, 0, 1)\n",
    "img_pos_transposed = img_pos.transpose(2, 0, 1)\n",
    "img_neg_transposed = img_neg.transpose(2, 0, 1)\n",
    "\n",
    "# set a pair data\n",
    "net.blobs['data'].data[...] = np.array([img_anc_transposed, img_pos_transposed])\n",
    "\n",
    "# calculate distance\n",
    "net.forward()\n",
    "\n",
    "dist = net.blobs['pos_dist'].data\n",
    "print('dist between anc and pos = {}'.format(dist))\n",
    "\n",
    "# set a pair data\n",
    "net.blobs['data'].data[...] = np.array([img_anc_transposed, img_neg_transposed])\n",
    "\n",
    "# calculate distance\n",
    "net.forward()\n",
    "\n",
    "dist = net.blobs['pos_dist'].data\n",
    "print('dist between anc and neg = {}'.format(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nループして、それぞれの距離を算出する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# number of iteration\n",
    "test_iter = 4988\n",
    "\n",
    "hard_triplets_indices = []\n",
    "semi_hard_triplets_indices = []\n",
    "easy_triplets_indices = []\n",
    "\n",
    "MNIST_ROOT = \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/images/\"\n",
    "\n",
    "for test_num in range(test_iter):\n",
    "    \n",
    "    print('test number: {}'.format(test_num))\n",
    "    \n",
    "    ANC_PATH = MNIST_ROOT + str(test_num) + \"_anchor.jpg\"\n",
    "    POS_PATH = MNIST_ROOT + str(test_num) + \"_positive.jpg\"\n",
    "    NEG_PATH = MNIST_ROOT + str(test_num) + \"_negative.jpg\"\n",
    "\n",
    "    img_anc = cv2.imread(ANC_PATH)\n",
    "    img_pos = cv2.imread(POS_PATH)\n",
    "    img_neg = cv2.imread(NEG_PATH)\n",
    "\n",
    "    img_anc_transposed = img_anc.transpose(2, 0, 1)\n",
    "    img_pos_transposed = img_pos.transpose(2, 0, 1)\n",
    "    img_neg_transposed = img_neg.transpose(2, 0, 1)\n",
    "\n",
    "    # calculate distance between anc and pos\n",
    "    net.blobs['data'].data[...] = np.array([img_anc_transposed, img_pos_transposed])\n",
    "    net.forward()\n",
    "    dist = net.blobs['pos_dist'].data\n",
    "    dist_p = copy.deepcopy(dist)\n",
    "    print('  dist between anc and pos = {}'.format(dist_p))\n",
    "\n",
    "    # calculate distance between anc and neg\n",
    "    net.blobs['data'].data[...] = np.array([img_anc_transposed, img_neg_transposed])\n",
    "    net.forward()\n",
    "    dist = net.blobs['pos_dist'].data\n",
    "    dist_n = copy.deepcopy(dist)\n",
    "    print('  dist between anc and neg = {}'.format(dist_n))\n",
    "    \n",
    "    # check result and append the test index to the hard triplets list if negative is closer than positive.\n",
    "    if dist_n < dist_p:\n",
    "        print('  hard triplet set')\n",
    "        hard_triplets_indices.append(test_num)\n",
    "        \n",
    "    elif dist_n < dist_p + 1.0: #1.0 = margin\n",
    "        print('  semi-hard triplet set')\n",
    "        semi_hard_triplets_indices.append(test_num)\n",
    "        \n",
    "    else:\n",
    "        print('  easy triplet set')\n",
    "        easy_triplets_indices.append(test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_triplets_list = np.array(hard_triplets_indices)\n",
    "print(\"the number of list = {}\".format(hard_triplets_list.shape[0]))\n",
    "hard_triplets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_hard_triplets_list = np.array(semi_hard_triplets_indices)\n",
    "print(\"the number of list = {}\".format(semi_hard_triplets_list.shape[0]))\n",
    "semi_hard_triplets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_triplets_list = np.array(easy_triplets_indices)\n",
    "print(\"the number of list = {}\".format(easy_triplets_list.shape[0]))\n",
    "easy_triplets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MNIST_ROOT = \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/images/\"\n",
    "\n",
    "for test_num in hard:\n",
    "    \n",
    "    print('test number: {}'.format(test_num))\n",
    "    \n",
    "    ANC_PATH = MNIST_ROOT + str(test_num) + \"_anchor.jpg\"\n",
    "    POS_PATH = MNIST_ROOT + str(test_num) + \"_positive.jpg\"\n",
    "    NEG_PATH = MNIST_ROOT + str(test_num) + \"_negative.jpg\"\n",
    "\n",
    "    img_anc = cv2.imread(ANC_PATH)\n",
    "    img_pos = cv2.imread(POS_PATH)\n",
    "    img_neg = cv2.imread(NEG_PATH)\n",
    "\n",
    "    img_anc_transposed = img_anc.transpose(2, 0, 1)\n",
    "    img_pos_transposed = img_pos.transpose(2, 0, 1)\n",
    "    img_neg_transposed = img_neg.transpose(2, 0, 1)\n",
    "\n",
    "    # calculate distance between anc and pos\n",
    "    net.blobs['data'].data[...] = np.array([img_anc_transposed, img_pos_transposed])\n",
    "    net.forward()\n",
    "    dist = net.blobs['pos_dist'].data\n",
    "    dist_p = copy.deepcopy(dist)\n",
    "    print('  dist between anc and pos = {}'.format(dist_p))\n",
    "\n",
    "    # calculate distance between anc and neg\n",
    "    net.blobs['data'].data[...] = np.array([img_anc_transposed, img_neg_transposed])\n",
    "    net.forward()\n",
    "    dist = net.blobs['pos_dist'].data\n",
    "    dist_n = copy.deepcopy(dist)\n",
    "    print('  dist between anc and neg = {}'.format(dist_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the distance matrix\n",
    "\n",
    "### `_pairwise_distances` の Numpy版実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _pairwise_distances(embeddings, squared=False):\n",
    "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    dot_product = np.matmul(embeddings, embeddings.transpose())\n",
    "    \n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    square_norm = np.diag(dot_product)\n",
    "    \n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = np.expand_dims(square_norm, 0) - 2.0 * dot_product + np.expand_dims(square_norm, 1)\n",
    "    \n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = np.maximum(distances, 0.0)\n",
    "    \n",
    "    if not squared:\n",
    "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
    "        # we need to add a small epsilon where distances == 0.0\n",
    "        mask = np.equal(distances, 0.0).astype(np.float)\n",
    "        distances = distances + mask * 1e-16\n",
    "\n",
    "        distances = np.sqrt(distances)\n",
    "\n",
    "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
    "        distances = distances * (1.0 - mask)\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.shapeで言うところの(batch_size, embed_dim)\n",
    "# 今回は(batch_size, embed_dim) = (3, 2)の例\n",
    "embeddings_example = np.array([[-1.0, 2.0],\n",
    "                               [0.5, 0.2],\n",
    "                               [5.5, 1.0]])    \n",
    "print embeddings_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pairwise_distances(embeddings_example, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベタ手法と比較\n",
    "for pair in [(0, 1), (0, 2), (1, 2)]:\n",
    "    dist = np.sum((embeddings_example[pair[0]] - embeddings_example[pair[1]])**2)\n",
    "    print('dist between {} = {}'.format(pair, dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `_get_triplet_mask` の Numpy版実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_triplet_mask(labels):\n",
    "    \"\"\"Return a 3D mask where mask[a, p, n] is True if the triplet (a, p, n) is valid.\n",
    "    A triplet (i, j, k) is valid if:\n",
    "        - i, j, k are distinct\n",
    "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i, j and k are distinct\n",
    "    indices_equal = np.eye(np.shape(labels)[0]).astype(np.bool)\n",
    "    indices_not_equal = np.logical_not(indices_equal)\n",
    "    i_not_equal_j = np.expand_dims(indices_not_equal, 2)\n",
    "    i_not_equal_k = np.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = np.expand_dims(indices_not_equal, 0)\n",
    "\n",
    "    distinct_indices = np.logical_and(np.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "\n",
    "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    label_equal = np.equal(np.expand_dims(labels, 0), np.expand_dims(labels, 1))\n",
    "    i_equal_j = np.expand_dims(label_equal, 2)\n",
    "    i_equal_k = np.expand_dims(label_equal, 1)\n",
    "\n",
    "    valid_labels = np.logical_and(i_equal_j, np.logical_not(i_equal_k))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = np.logical_and(distinct_indices, valid_labels)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test1 = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "_get_triplet_mask(label_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test2 = np.array([1, 2, 1, 4, 5])\n",
    "\n",
    "_get_triplet_mask(label_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test3 = np.array([1, 2, 1, 2, 3])\n",
    "\n",
    "_get_triplet_mask(label_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test4 = np.array([1, 1, 1, 1, 1])\n",
    "\n",
    "_get_triplet_mask(label_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `batch_all_triplet_loss` の Numpy版実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_all_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    We generate all the valid triplets and average the loss over the positive ones.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    anchor_positive_dist = np.expand_dims(pairwise_dist, 2)\n",
    "    anchor_negative_dist = np.expand_dims(pairwise_dist, 1)\n",
    "    \n",
    "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
    "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
    "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
    "    # and the 2nd (batch_size, 1, batch_size)\n",
    "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
    "    \n",
    "    # Put to zero the invalid triplets\n",
    "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
    "    mask = _get_triplet_mask(labels)\n",
    "    mask = mask.astype(np.float)\n",
    "    triplet_loss = np.multiply(mask, triplet_loss)\n",
    "    \n",
    "    # Remove negative losses (i.e. the easy triplets)\n",
    "    triplet_loss = np.maximum(triplet_loss, 0.0)\n",
    "    \n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    valid_triplets = np.greater(triplet_loss, 1e-16).astype(float)\n",
    "    num_positive_triplets = np.sum(valid_triplets)\n",
    "    num_valid_triplets = np.sum(mask)\n",
    "    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + 1e-16)\n",
    "\n",
    "    # Get final mean triplet loss over the positive valid triplets\n",
    "    triplet_loss = np.sum(triplet_loss) / (num_positive_triplets + 1e-16)\n",
    "\n",
    "    return triplet_loss, fraction_positive_triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test the triplet loss with batch all triplet mining in a simple case.\n",
    "   There is just one class in this super simple edge case, and we want to make sure that\n",
    "   the loss is 0.\n",
    "\"\"\"\n",
    "num_data = 10\n",
    "feat_dim = 6\n",
    "margin = 0.2\n",
    "num_classes = 1\n",
    "\n",
    "embeddings = np.random.rand(num_data, feat_dim).astype(np.float32)\n",
    "labels = np.random.randint(0, num_classes, size=(num_data)).astype(np.float32)\n",
    "\n",
    "for squared in [True, False]:\n",
    "    print('#{}'.format(squared))\n",
    "    loss_np = 0.0\n",
    "\n",
    "    # Compute the loss in TF.\n",
    "    loss_tf, fraction = batch_all_triplet_loss(labels, embeddings, margin, squared=squared)\n",
    "    \n",
    "    print('  loss_tf  = {}'.format(loss_tf))\n",
    "    print('  fraction = {}'.format(fraction))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance_np(feature, squared=False):\n",
    "    \"\"\"Computes the pairwise distance matrix in numpy.\n",
    "    Args:\n",
    "        feature: 2-D numpy array of size [number of data, feature dimension]\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean\n",
    "                 distance matrix; else, output is the pairwise euclidean distance matrix.\n",
    "    Returns:\n",
    "        pairwise_distances: 2-D numpy array of size\n",
    "                            [number of data, number of data].\n",
    "    \"\"\"\n",
    "    triu = np.triu_indices(feature.shape[0], 1)\n",
    "    upper_tri_pdists = np.linalg.norm(feature[triu[1]] - feature[triu[0]], axis=1)\n",
    "    if squared:\n",
    "        upper_tri_pdists **= 2.\n",
    "    num_data = feature.shape[0]\n",
    "    pairwise_distances = np.zeros((num_data, num_data))\n",
    "    pairwise_distances[np.triu_indices(num_data, 1)] = upper_tri_pdists\n",
    "    # Make symmetrical.\n",
    "    pairwise_distances = pairwise_distances + pairwise_distances.T - np.diag(\n",
    "            pairwise_distances.diagonal())\n",
    "    return pairwise_distances\n",
    "\n",
    "\n",
    "\"\"\"Test the triplet loss with batch all triplet mining\"\"\"\n",
    "num_data = 10\n",
    "feat_dim = 6\n",
    "margin = 0.2\n",
    "num_classes = 5\n",
    "\n",
    "embeddings = np.random.rand(num_data, feat_dim).astype(np.float32)\n",
    "labels = np.random.randint(0, num_classes, size=(num_data)).astype(np.float32)\n",
    "\n",
    "for squared in [True, False]:\n",
    "    print('#{}'.format(squared))\n",
    "    pdist_matrix = pairwise_distance_np(embeddings, squared=squared)\n",
    "    print(pdist_matrix.shape)\n",
    "\n",
    "    loss_np = 0.0\n",
    "    num_positives = 0.0\n",
    "    num_valid = 0.0\n",
    "    for i in range(num_data):\n",
    "        for j in range(num_data):\n",
    "            for k in range(num_data):\n",
    "                distinct = (i != j and i != k and j != k)\n",
    "                valid = (labels[i] == labels[j]) and (labels[i] != labels[k])\n",
    "                if distinct and valid:\n",
    "                    num_valid += 1.0\n",
    "\n",
    "                    pos_distance = pdist_matrix[i][j]\n",
    "                    neg_distance = pdist_matrix[i][k]\n",
    "\n",
    "                    loss = np.maximum(0.0, pos_distance - neg_distance + margin)\n",
    "                    loss_np += loss\n",
    "\n",
    "                    num_positives += (loss > 0)\n",
    "\n",
    "    loss_np /= num_positives\n",
    "\n",
    "    # Compute the loss in TF.\n",
    "    loss_tf, fraction = batch_all_triplet_loss(labels, embeddings, margin, squared=squared)\n",
    "    print('  loss_tf  = {}'.format(loss_tf))\n",
    "    print('  fraction = {}'.format(fraction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `batch_hard_triplet_loss` の Numpy版実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i and j are distinct\n",
    "    indices_equal = np.eye(np.shape(labels)[0]).astype(np.bool)\n",
    "    indices_not_equal = np.logical_not(indices_equal)\n",
    "\n",
    "    # Check if labels[i] == labels[j]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = np.equal(np.expand_dims(labels, 0), np.expand_dims(labels, 1))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = np.logical_and(indices_not_equal, labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = np.equal(np.expand_dims(labels, 0), np.expand_dims(labels, 1))\n",
    "\n",
    "    mask = np.logical_not(labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # For each anchor, get the hardest positive\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "    mask_anchor_positive = (mask_anchor_positive).astype(np.float)\n",
    "\n",
    "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
    "    anchor_positive_dist = np.multiply(mask_anchor_positive, pairwise_dist)\n",
    "\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dist = np.amax(anchor_positive_dist, axis=1, keepdims=True)\n",
    "\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    mask_anchor_negative = (mask_anchor_negative).astype(np.float)\n",
    "\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    max_anchor_negative_dist = np.amax(pairwise_dist, axis=1, keepdims=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "\n",
    "    # shape (batch_size,)\n",
    "    hardest_negative_dist = np.amin(anchor_negative_dist, axis=1, keepdims=True)\n",
    "\n",
    "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "    triplet_loss = np.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
    "\n",
    "    # Get final mean triplet loss\n",
    "    triplet_loss = np.mean(triplet_loss)\n",
    "\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test the triplet loss with batch hard triplet mining\"\"\"\n",
    "num_data = 4\n",
    "feat_dim = 3\n",
    "margin = 0.2\n",
    "num_classes = 5\n",
    "\n",
    "embeddings = np.random.rand(num_data, feat_dim).astype(np.float32)\n",
    "labels = np.random.randint(0, num_classes, size=(num_data)).astype(np.float32)\n",
    "\n",
    "for squared in [True, False]:\n",
    "    print('#{}'.format(squared))\n",
    "    pdist_matrix = pairwise_distance_np(embeddings, squared=squared)\n",
    "\n",
    "    loss_np = 0.0\n",
    "    for i in range(num_data):\n",
    "        # Select the hardest positive\n",
    "        max_pos_dist = np.max(pdist_matrix[i][labels == labels[i]])\n",
    "\n",
    "        # Select the hardest negative\n",
    "        min_neg_dist = np.min(pdist_matrix[i][labels != labels[i]])\n",
    "\n",
    "        loss = np.maximum(0.0, max_pos_dist - min_neg_dist + margin)\n",
    "        loss_np += loss\n",
    "\n",
    "    loss_np /= num_data\n",
    "\n",
    "    # Compute the loss in TF.\n",
    "    loss_tf = batch_hard_triplet_loss(labels, embeddings, margin, squared=squared)\n",
    "    print('  loss_tf  = {}'.format(loss_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_data = np.loadtxt(\"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/trainlist_64.txt\",usecols=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_labels = txt_data[:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change 'test_num', then you will get images and the distances of the coressponding test set number.\n",
    "test_num = 0\n",
    "mnist_embeddings = np.zeros((64,10))\n",
    "\n",
    "for test_num in range(64):\n",
    "    MNIST_ROOT = \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/images/\"\n",
    "    ANC_PATH = MNIST_ROOT + str(test_num) + \"_anchor.jpg\"\n",
    "    POS_PATH = MNIST_ROOT + str(test_num) + \"_positive.jpg\"\n",
    "    NEG_PATH = MNIST_ROOT + str(test_num) + \"_negative.jpg\"\n",
    "\n",
    "    #\n",
    "    img_anc = cv2.imread(ANC_PATH)\n",
    "    img_pos = cv2.imread(POS_PATH)\n",
    "    img_neg = cv2.imread(NEG_PATH)\n",
    "\n",
    "    plt.imshow(img_anc)\n",
    "\n",
    "    img_anc_transposed = img_anc.transpose(2, 0, 1)\n",
    "    img_pos_transposed = img_pos.transpose(2, 0, 1)\n",
    "    img_neg_transposed = img_neg.transpose(2, 0, 1)\n",
    "\n",
    "    # set a pair data\n",
    "    net.blobs['data'].data[...] = np.array([img_anc_transposed, img_pos_transposed])\n",
    "\n",
    "    # calculate distance\n",
    "    net.forward()\n",
    "\n",
    "    mnist_embeddings[test_num] = net.blobs['feat'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist_labels)\n",
    "print(mnist_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = mnist_labels\n",
    "embeddings = mnist_embeddings\n",
    "\n",
    "for squared in [True, False]:\n",
    "    print('squared=#{}'.format(squared))\n",
    "    pdist_matrix = pairwise_distance_np(embeddings, squared=squared)\n",
    "\n",
    "    loss_np = 0.0\n",
    "    for i in range(num_data):\n",
    "        # Select the hardest positive\n",
    "        max_pos_dist = np.max(pdist_matrix[i][labels == labels[i]])\n",
    "\n",
    "        # Select the hardest negative\n",
    "        min_neg_dist = np.min(pdist_matrix[i][labels != labels[i]])\n",
    "\n",
    "        loss = np.maximum(0.0, max_pos_dist - min_neg_dist + margin)\n",
    "        loss_np += loss\n",
    "\n",
    "    loss_np /= num_data\n",
    "\n",
    "    # Compute the loss in TF.\n",
    "    loss_tf = batch_hard_triplet_loss(labels, embeddings, margin, squared=squared)\n",
    "    print('  loss_tf  = {}'.format(loss_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Triplet Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## omoindrot's blog  \n",
    "### https://omoindrot.github.io/triplet-loss#offline-and-online-triplet-mining\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "omoindrotのOnline triplet miningは\n",
    "http://bamos.github.io/2016/01/19/openface-0.2.0/\n",
    "の思想に基づいている。\n",
    "\n",
    "Bartoszの洞察は、ネットワークを共有パラメータで複製する必要がなく、組み込みを三つ組にマッピングすることによって、独自の画像上で単一のネットワークを使用できること。\n",
    "\n",
    "\n",
    "![説明図](https://omoindrot.github.io/assets/triplet_loss/online_triplet_loss.png \"説明図\")\n",
    "\n",
    "![説明図](http://bamos.github.io/data/2016-01-19/optimization-after.png \"説明図\")\n",
    "\n",
    "データセット内の15人から1人あたり20枚の画像をサンプリングし、ネットワークを介して300枚の画像すべてをGPU上で1回の順方向パスで送信して、300個の埋め込みを取得している。 次に、CPU上で、これらの埋め込みが2850トリプレットにマッピングされ、triplet関数に渡される。その後、微分は逆伝播でネットワークパスの元の画像へ戻ってマッピングされる。こうすれば1回でトリプレットの計算が済む。 \n",
    "\n",
    "\n",
    "👉モデルは、siameseの3並列にする必要はなくなり、1つの単一のネットワークで事足りてしまうようになる。  \n",
    "\n",
    "[課題]  \n",
    "1.  どのようにネットワークを組むべきか。 \n",
    "2.  また、逆伝播はどのように実装しているか・・・？Caffeでは手計算しかなさそう。  \n",
    "https://arxiv.org/pdf/1703.07737.pdf　式(5)を微分して実装することになるか。\n",
    "もしくは、http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1FilterLayer.html　をうまくつかえばできる？？\n",
    "\n",
    "👉いけそうな未来が見えた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "以下がトリプレットロスのコード。\n",
    "\n",
    "-----\n",
    "\n",
    "```python\n",
    "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    \n",
    "#1.\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "#2.(a-pマスク)\n",
    "    # For each anchor, get the hardest positive\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "    mask_anchor_positive = tf.to_float(mask_anchor_positive)\n",
    "    \n",
    "#3.\n",
    "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
    "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
    "\n",
    "#5.\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
    "\n",
    "#2.(a-nマスク)\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    mask_anchor_negative = tf.to_float(mask_anchor_negative)\n",
    "\n",
    "#4.\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "\n",
    "#6.\n",
    "    # shape (batch_size,)\n",
    "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
    "\n",
    "#7.\n",
    "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
    "\n",
    "    # Get final mean triplet loss\n",
    "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
    "\n",
    "    return triplet_loss \n",
    "```\n",
    "\n",
    "-----\n",
    "やることをまとめると、以下。\n",
    "\n",
    "1.  embedding出力層のoutから、全embeddeingの組合せの Distance を導出したマトリクスを作る\n",
    "2.  labelから、a-pマスク・a-nマスクを作る。マスクはbackpropagationいらない。\n",
    "3.  マトリクスとa-pマスクで、positive-distanceのみ残す。(filter layer)\n",
    "4.  マトリクスとa-nマスクで、negative-distanceのみ残す。(filter layer)\n",
    "\n",
    "5.  3.の出力をargmax layerで最大値のみ残す。次元削減\n",
    "6.  4.の出力を x(-1) -> argmax -> x(-1) する。　(argminがない)\n",
    "\n",
    "7.  triplet loss を各anchorに対して計算したのち　mean を出す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネットワークを設計すると　以下のようになる？\n",
    "\n",
    "![network](online_mining_network.png \"online mining network\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caffe の filter layerについて、mask側にはbackpropagationできない。\n",
    "mask生成するpython layerが必要になりそうだが、ここでbackwardを実装する必要はないはず。\n",
    "よって、mask生成プログラムのnp版がそのまま使えるはず。\n",
    "\n",
    "pairwise distanceは、うまく今のレイヤを流用して作成する必要がありそう。\n",
    "（既存のレイヤについて、出力が配列要素になるように改造すれば良いだけか？）\n",
    "\n",
    "Triplet Lossも同様に、バッチサイズ分の次元の入力に対して出力し、meanを導出するというプロセスが必要。\n",
    "\n",
    "negative側にはargmin layer なるものがなかったので、powerで-1倍するレイヤを挟むことによって実現する。\n",
    "\n",
    "\n",
    "## 以下、まずはpython layerを作成するところから。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pairwise distances layer (作成中)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tripletloss/pairwise_distances_layer.py\n",
    "import caffe\n",
    "import numpy as np\n",
    "\n",
    "class PairwiseDistancesLayer(caffe.Layer):\n",
    "    \"\"\"\n",
    "    Compute the Triplet Loss based on the Google's FaceNet paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self, bottom, top):\n",
    "        pass\n",
    "\n",
    "    def reshape(self, bottom, top):\n",
    "        print('# reshape start.')\n",
    "        \n",
    "        # self.batch_size: number of batch_size. [1]\n",
    "        self.batch_size = bottom[0].data.shape[0]\n",
    "        print('batch_size = {}'.format(self.batch_size))\n",
    "        \n",
    "        # self.diff: differences are shape of channel [channel]\n",
    "        self.diff = np.zeros_like(bottom[0].data.shape[1], dtype=np.float32)\n",
    "        print('diff = {}'.format(self.diff))\n",
    "        \n",
    "        # self.dist: distance is scalar [1]\n",
    "        self.dist = np.zeros(1, dtype=np.float32)\n",
    "        print('dist = {}'.format(self.dist))\n",
    "        \n",
    "        # normalize (# I still keep using this function.)\n",
    "        self.norm = self.normalize(bottom[0].data)\n",
    "        print('norm = \\n{}'.format(self.norm))\n",
    "        \n",
    "        # pairwise distances output with shape [batch_size, batch_size]\n",
    "        top[0].reshape(bottom[0].data.shape[0], bottom[0].data.shape[0])\n",
    "        print('top[0] data shape = {}'.format(top[0].data.shape))\n",
    "        \n",
    "        print('# reshape end.')\n",
    "        \n",
    "    def normalize(self, array):\n",
    "        # ||f(x)||_2=1\n",
    "        l2 = np.linalg.norm(array, ord=2, axis=1, keepdims=True)\n",
    "        # avoid to devide by zero\n",
    "        l2[l2==0] = 1\n",
    "        return array / l2\n",
    "\n",
    "    def forward(self, bottom, top):\n",
    "        \"\"\" computes a loss\n",
    "        Note that the Loss is not averaged by the number of triplet sets.\n",
    "        Loss = SUM[i->N](Di_pos - Di_neg + margin), 0 <= i <= N(the batch size)\n",
    "        Dpos = sqrt(L2(IMGi_acr - IMGi_pos))\n",
    "        Dneg = sqrt(L2(IMGi_acr - IMGi_neg))\n",
    "        \"\"\"\n",
    "        \n",
    "        print('# forward start.')\n",
    "        \n",
    "        for i in range(0, self.batch_size):\n",
    "            for j in range(0, i):                \n",
    "                print('i = {}'.format(i))\n",
    "                print('j = {}'.format(j))\n",
    "                print('self.norm = \\n{}'.format(self.norm[i]))\n",
    "                \n",
    "                self.diff = self.norm[i] - self.norm[j]\n",
    "                print('self.diff = \\n{}'.format(self.diff))\n",
    "                self.dist = np.sum(self.diff**2, axis=0)\n",
    "                print('self.dist = {}'.format(self.dist))\n",
    "\n",
    "                top[0].data[i,j] = self.dist\n",
    "                top[0].data[j,i] = self.dist\n",
    "                \n",
    "        print('top[0] data = \\n{}'.format(top[0].data))\n",
    "        print('# forward end.')\n",
    "\n",
    "    def backward(self, top, propagate_down, bottom): #now under constract.\n",
    "        \"\"\" computes a gradient w.r.t. each IMG\n",
    "        dL/dDorg = SUM[i->N]{2(IMGi_neg - IMGi_pos)} if Lossi > 0 else 0\n",
    "        dL/dDpos = SUM[i->N](-2(IMGi_anc - IMGi_pos)) if Lossi > 0 else 0\n",
    "        dL/dDneg = SUM[i->N](2(IMGi_anc - IMGi_neg)) if Lossi > 0 else 0\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "#        # gradient w.r.t. Dorg\n",
    "#        diff_org = self.norm_neg - self.norm_pos\n",
    "#        for i in range(self.batch_size):\n",
    "#            if self.loss[i] == 0:\n",
    "#                diff_org[i] = 0\n",
    "#        bottom[0].diff[...] = 2 * diff_org\n",
    "#        print('org diff = {}'.format(bottom[0].diff))\n",
    "#        \n",
    "#        # gradient w.r.t. Dpos\n",
    "#        for i in range(self.batch_size):\n",
    "#            self.diff_pos[i] = 0\n",
    "#        bottom[1].diff[...] = -2 * self.diff_pos\n",
    "#        print('pos diff = {}'.format(bottom[1].diff))\n",
    "#        \n",
    "#        # gradient w.r.t. Dneg\n",
    "#        for i in range(self.batch_size):\n",
    "#            self.diff_neg[i] = 0\n",
    "#        bottom[2].diff[...] = 2 * self.diff_neg\n",
    "#        print('neg diff = {}'.format(bottom[2].diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caffe_root = '../'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "sys.path.insert(0, caffe_root + 'examples/tripletloss')\n",
    "import caffe\n",
    "\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from caffe import layers as L\n",
    "\n",
    "def load_net(net_proto):\n",
    "    f = tempfile.NamedTemporaryFile(mode='w+', delete=False)\n",
    "    f.write(str(net_proto))\n",
    "    f.close()\n",
    "    return caffe.Net(f.name, caffe.TEST)\n",
    "\n",
    "def example_network(batch_size):\n",
    "    n = caffe.NetSpec()\n",
    "\n",
    "    # we use the dummy data layer to control the \n",
    "    # shape of the inputs to the layer we are testing\n",
    "    ip_dims = [batch_size, 3]\n",
    "    n.ip = L.DummyData(shape=[dict(dim=ip_dims)],\n",
    "                                        transform_param=dict(scale=1.0/255.0),\n",
    "                                        ntop=1)\n",
    "    \n",
    "    n.a_p_mask = L.Python(n.ip, ntop=1, python_param=dict(module='pairwise_distances_layer', layer='PairwiseDistancesLayer'))\n",
    "    return n.to_proto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ANC = [[1.0, 2.0, 3.0],\n",
    "           [2.0, 3.0, 4.0],\n",
    "           [3.0, 4.0, 5.0],\n",
    "           [4.0, 5.0, 6.0],\n",
    "           [5.0, 6.0, 7.0],\n",
    "           [6.0, 7.0, 8.0]]\n",
    "           \n",
    "# embeddings is an 1D-array of features\n",
    "# here, the size of features is 3, 3*32bit = 96bit\n",
    "# (batch_size, feature_size)\n",
    "embeddings = np.array([IMG_ANC], dtype=np.float32)\n",
    "print('embeddings shape = {}'.format(embeddings.shape))\n",
    "\n",
    "net_proto = example_network(6)\n",
    "net = load_net(net_proto)\n",
    "net.blobs['ip'].data[...] = embeddings\n",
    "\n",
    "net.forward()\n",
    "\n",
    "for name in net.blobs:\n",
    "    print('# {}'.format(name))\n",
    "    print('value = \\n{}'.format(net.blobs[name].data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "net.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean triplet loss layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tripletloss/mean_triplet_loss_layer.py\n",
    "import caffe\n",
    "import numpy as np\n",
    "\n",
    "class MeanTripletLossLayer(caffe.Layer):\n",
    "    \"\"\"\n",
    "    Compute the Triplet Loss based on the Google's FaceNet paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self, bottom, top):\n",
    "        print('# setup start.')\n",
    "        \n",
    "        params = eval(self.param_str)\n",
    "        try:\n",
    "            self.margin = float(params['margin'])\n",
    "        except:\n",
    "            self.margin = 1.0\n",
    "            \n",
    "        print('margin = {}'.format(self.margin))\n",
    "        print('# setup end.')\n",
    "\n",
    "    def reshape(self, bottom, top):\n",
    "        print('# reshape start.')\n",
    "        \n",
    "        # self.hardest_pos: [batch size]\n",
    "        self.hardest_pos = np.zeros_like(bottom[0].data, dtype=np.float32)\n",
    "        print('self.hardest_pos = {}'.format(self.hardest_pos))\n",
    "        \n",
    "        # self.hardest_neg: [batch size]\n",
    "        self.hardest_neg = np.zeros_like(bottom[0].data, dtype=np.float32)\n",
    "        print('self.hardest_neg = {}'.format(self.hardest_neg))\n",
    "        \n",
    "        # self.losses: [batch size]\n",
    "        self.losses = np.zeros_like(bottom[0].data, dtype=np.float32)\n",
    "        print('self.losses = {}'.format(self.losses))\n",
    "        \n",
    "        # pairwise distances output with shape [batch_size, batch_size]\n",
    "        top[0].reshape(1)\n",
    "        print('top[0] data shape = {}'.format(top[0].data.shape))\n",
    "        \n",
    "        print('# reshape end.')\n",
    "        \n",
    "    def forward(self, bottom, top):\n",
    "        \"\"\" computes a loss\n",
    "        Note that the Loss is not averaged by the number of triplet sets.\n",
    "        Loss = SUM[i->N](Di_pos - Di_neg + margin), 0 <= i <= N(the batch size)\n",
    "        Dpos = sqrt(L2(IMGi_acr - IMGi_pos))\n",
    "        Dneg = sqrt(L2(IMGi_acr - IMGi_neg))\n",
    "        \"\"\"\n",
    "        print('# forward start.')\n",
    "        \n",
    "        self.hardest_pos = bottom[0].data\n",
    "        print('self.hardest_pos = {}'.format(self.hardest_pos))\n",
    "        \n",
    "        self.hardest_neg = bottom[1].data\n",
    "        print('self.hardest_neg = {}'.format(self.hardest_neg))\n",
    "        \n",
    "        # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "        self.losses = np.maximum(self.hardest_pos - self.hardest_neg + self.margin, 0.0)\n",
    "        print('self.losses = {}'.format(self.losses))\n",
    "\n",
    "        # Get final mean triplet loss\n",
    "        top[0].data[...] = np.mean(self.losses)\n",
    "        print('loss = {}'.format(top[0].data))\n",
    "\n",
    "        print('# forward end.')\n",
    "\n",
    "    def backward(self, top, propagate_down, bottom): #now under constraction.\n",
    "        \"\"\" computes a gradient w.r.t. each IMG\n",
    "        dL/dDorg = SUM[i->N]{2(IMGi_neg - IMGi_pos)} if Lossi > 0 else 0\n",
    "        dL/dDpos = SUM[i->N](-2(IMGi_anc - IMGi_pos)) if Lossi > 0 else 0\n",
    "        dL/dDneg = SUM[i->N](2(IMGi_anc - IMGi_neg)) if Lossi > 0 else 0\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "#        # gradient w.r.t. Dorg\n",
    "#        diff_org = self.norm_neg - self.norm_pos\n",
    "#        for i in range(self.batch_size):\n",
    "#            if self.loss[i] == 0:\n",
    "#                diff_org[i] = 0\n",
    "#        bottom[0].diff[...] = 2 * diff_org\n",
    "#        print('org diff = {}'.format(bottom[0].diff))\n",
    "#        \n",
    "#        # gradient w.r.t. Dpos\n",
    "#        for i in range(self.batch_size):\n",
    "#            self.diff_pos[i] = 0\n",
    "#        bottom[1].diff[...] = -2 * self.diff_pos\n",
    "#        print('pos diff = {}'.format(bottom[1].diff))\n",
    "#        \n",
    "#        # gradient w.r.t. Dneg\n",
    "#        for i in range(self.batch_size):\n",
    "#            self.diff_neg[i] = 0\n",
    "#        bottom[2].diff[...] = 2 * self.diff_neg\n",
    "#        print('neg diff = {}'.format(bottom[2].diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caffe_root = '../'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "sys.path.insert(0, caffe_root + 'examples/tripletloss')\n",
    "import caffe\n",
    "\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from caffe import layers as L\n",
    "\n",
    "def load_net(net_proto):\n",
    "    f = tempfile.NamedTemporaryFile(mode='w+', delete=False)\n",
    "    f.write(str(net_proto))\n",
    "    f.close()\n",
    "    return caffe.Net(f.name, caffe.TEST)\n",
    "\n",
    "def example_network(batch_size):\n",
    "    n = caffe.NetSpec()\n",
    "\n",
    "    # we use the dummy data layer to control the \n",
    "    # shape of the inputs to the layer we are testing\n",
    "    pos_dims = [batch_size]\n",
    "    n.pos = L.DummyData(shape=[dict(dim=pos_dims)],\n",
    "                                        transform_param=dict(scale=1.0/255.0),\n",
    "                                        ntop=1)\n",
    "    neg_dims = [batch_size]\n",
    "    n.neg = L.DummyData(shape=[dict(dim=neg_dims)],\n",
    "                                        transform_param=dict(scale=1.0/255.0),\n",
    "                                        ntop=1)\n",
    "    \n",
    "    n.loss = L.Python(n.pos, n.neg, ntop=1, python_param=dict(module='mean_triplet_loss_layer', layer='MeanTripletLossLayer', param_str='{\\\"margin\\\": 2.0}'))\n",
    "    return n.to_proto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_POS = [1.0, 2.0, 3.0, 3.0, 2.0, 1.0]\n",
    "IMG_NEG = [3.0, 2.0, 1.0, 5.0, 3.0, 0.0]\n",
    "\n",
    "# embeddings is an 1D-array of features\n",
    "# here, the size of features is 3, 3*32bit = 96bit\n",
    "# (batch_size, feature_size)\n",
    "positives = np.array([IMG_POS], dtype=np.float32)\n",
    "negatives = np.array([IMG_NEG], dtype=np.float32)\n",
    "\n",
    "net_proto = example_network(6)\n",
    "net = load_net(net_proto)\n",
    "net.blobs['pos'].data[...] = positives\n",
    "net.blobs['neg'].data[...] = negatives\n",
    "\n",
    "net.forward()\n",
    "\n",
    "for name in net.blobs:\n",
    "    print('# {}'.format(name))\n",
    "    print('value = \\n{}'.format(net.blobs[name].data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "net.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anchor-positive & anchor-negative mask layer (完成！)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tripletloss/triplet_mask_layer.py\n",
    "import caffe\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TripletMaskLayer(caffe.Layer):\n",
    "    \"\"\"\n",
    "    Compute matrix shaped a-p mask and a-n mask.\n",
    "    this layer is forward only.\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self, bottom, top):\n",
    "        print('# setup start.')\n",
    "        print('# setup end.')\n",
    "\n",
    "    def reshape(self, bottom, top):\n",
    "        print('# reshape start.')\n",
    "        \n",
    "        # Check that i and j are distinct\n",
    "        self.indices_equal = np.eye(bottom[0].data.shape[0]).astype(np.bool)\n",
    "        self.indices_not_equal = np.logical_not(self.indices_equal)\n",
    "        self.labels_equal = np.zeros((bottom[0].data.shape[0], bottom[0].data.shape[0])).astype(np.bool)\n",
    "        \n",
    "        \n",
    "        print('bottom data shape = {}'.format(bottom[0].data.shape))\n",
    "        print('indices_equal = \\n{}'.format(self.indices_equal))\n",
    "        print('indices_not_equal = \\n{}'.format(self.indices_not_equal))\n",
    "        print('labels_equal = \\n{}'.format(self.labels_equal))\n",
    "        \n",
    "        # anchor-positive mask output with shape [batch_size, batch_size]\n",
    "        top[0].reshape(bottom[0].data.shape[0], bottom[0].data.shape[0])\n",
    "        print('top[0] data shape = {}'.format(top[0].data.shape))\n",
    "        \n",
    "        # anchor-negative mask output with shape [batch_size, batch_size]\n",
    "        top[1].reshape(bottom[0].data.shape[0], bottom[0].data.shape[0])\n",
    "        print('top[1] data shape = {}'.format(top[1].data.shape))\n",
    "      \n",
    "        print('# reshape end.')\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, bottom, top):\n",
    "        \"\"\"1. make a 2D mask where mask[a, p] is True if a and p are distinct and have same label.\n",
    "           2. make a 2D mask where mask[a, n] is True if a and n have distinct labels.\n",
    "        Args:\n",
    "            labels: np.int32 `ndarray` with shape [batch_size]\n",
    "        Returns:\n",
    "            2 masks: np.bool `ndarray` with shape [batch_size, batch_size]\n",
    "            top[0]: anchor-positive mask.\n",
    "            top[1]: anchor-negative mask.\n",
    "        \"\"\"\n",
    "        print('# forward start.')\n",
    "        \n",
    "        # Check if labels[i] == labels[j]\n",
    "        # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "        self.labels_equal = np.equal(np.expand_dims(bottom[0].data, 0), np.expand_dims(bottom[0].data, 1))\n",
    "        \n",
    "        print('labels_equal = \\n{}'.format(self.labels_equal))\n",
    "        \n",
    "        # Combine the two masks\n",
    "        top[0].data[...] = np.logical_and(self.indices_not_equal, self.labels_equal)\n",
    "        top[1].data[...] = np.logical_not(self.labels_equal)\n",
    "        \n",
    "        print('a-p mask = \\n{}'.format(top[0].data))\n",
    "        print('a-n mask = \\n{}'.format(top[0].data))\n",
    "        \n",
    "        print('# forward end.')\n",
    "\n",
    "    def backward(self, top, propagate_down, bottom):\n",
    "        # this layer is forward only.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "マスクレイヤが正しく動いているかをチェック。Hello test forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caffe_root = '../'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "sys.path.insert(0, caffe_root + 'examples/tripletloss')\n",
    "import caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import numpy as np\n",
    "from caffe import layers as L\n",
    "\n",
    "def load_net(net_proto):\n",
    "    f = tempfile.NamedTemporaryFile(mode='w+', delete=False)\n",
    "    f.write(str(net_proto))\n",
    "    f.close()\n",
    "    return caffe.Net(f.name, caffe.TEST)\n",
    "\n",
    "def example_network(batch_size):\n",
    "    n = caffe.NetSpec()\n",
    "\n",
    "    # we use the dummy data layer to control the \n",
    "    # shape of the inputs to the layer we are testing\n",
    "    ip_dims = [batch_size, 1]\n",
    "    label_dims = [batch_size]\n",
    "    n.label = L.DummyData(shape=[dict(dim=label_dims)],\n",
    "                                 transform_param=dict(scale=1.0/255.0),\n",
    "                                 ntop=1)\n",
    "    \n",
    "    n.a_p_mask, n.a_n_mask = L.Python(n.label, ntop=2, python_param=dict(module='triplet_mask_layer', layer='TripletMaskLayer'))\n",
    "    return n.to_proto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_LABEL = [1, 2, 3, 4, 2, 2]\n",
    "\n",
    "label_data = np.array(IMG_LABEL)\n",
    "print('label_data shape = {}'.format(label_data.shape))\n",
    "\n",
    "\n",
    "net_proto = example_network(label_data.shape[0])\n",
    "net = load_net(net_proto)\n",
    "net.blobs['label'].data[...] = label_data\n",
    "net.forward()\n",
    "\n",
    "for name in net.blobs:\n",
    "    print('# {}'.format(name))\n",
    "    print('value = \\n{}'.format(net.blobs[name].data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Networkを設計する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tripletloss/mnist_omoindrot_tripletloss_train_test_10.prototxt\n",
    "name: \"mnist_tripletloss_train_test_10\"\n",
    "layer {\n",
    "  name: \"triplet_data\"\n",
    "  type: \"ImageData\"\n",
    "  top: \"triplet_data\"\n",
    "  top: \"label\"\n",
    "  include {\n",
    "    phase: TRAIN\n",
    "  }\n",
    "  transform_param {\n",
    "    scale: 0.00390625\n",
    "  }\n",
    "  image_data_param {\n",
    "    source: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/trainlist_64.txt\"\n",
    "    batch_size: 64\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"triplet_data\"\n",
    "  type: \"ImageData\"\n",
    "  top: \"triplet_data\"\n",
    "  top: \"label\"\n",
    "  include {\n",
    "    phase: TEST\n",
    "  }\n",
    "  transform_param {\n",
    "    scale: 0.00390625\n",
    "  }\n",
    "  image_data_param {\n",
    "    source: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/trainlist_64.txt\"\n",
    "    batch_size: 64\n",
    "  }\n",
    "}\n",
    "################# CNN #############\n",
    "layer {\n",
    "  name: \"conv1\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"triplet_data\"\n",
    "  top: \"conv1\"\n",
    "  param {\n",
    "    name: \"conv1_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"conv1_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 20\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool1\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv1\"\n",
    "  top: \"pool1\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv2\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"pool1\"\n",
    "  top: \"conv2\"\n",
    "  param {\n",
    "    name: \"conv2_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"conv2_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 50\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool2\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv2\"\n",
    "  top: \"pool2\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"ip1\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"pool2\"\n",
    "  top: \"ip1\"\n",
    "  param {\n",
    "    name: \"ip1_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"ip1_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 500\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu1\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"ip1\"\n",
    "  top: \"ip1\"\n",
    "}\n",
    "layer {\n",
    "  name: \"ip2\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"ip1\"\n",
    "  top: \"feat\"\n",
    "  param {\n",
    "    name: \"ip2_w\"\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    name: \"ip2_b\"\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 10\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "############# L2 Normalization ############\n",
    "\n",
    "layer {\n",
    "  name: \"Reduction1\"\n",
    "  type: \"Reduction\"\n",
    "  bottom: \"feat\"\n",
    "  top: \"Reduction1\"\n",
    "  reduction_param {\n",
    "    operation: SUMSQ\n",
    "    axis: 1\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"Power1\"\n",
    "  type: \"Power\"\n",
    "  bottom: \"Reduction1\"\n",
    "  top: \"Power1\"\n",
    "  power_param {\n",
    "    power: -0.5\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"Reshape1\"\n",
    "  type: \"Reshape\"\n",
    "  bottom: \"Power1\"\n",
    "  top: \"Reshape1\"\n",
    "  reshape_param {\n",
    "    shape {\n",
    "      dim: 1\n",
    "    }\n",
    "    axis: -1\n",
    "    num_axes: 0\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"Tile1\"\n",
    "  type: \"Tile\"\n",
    "  bottom: \"Reshape1\"\n",
    "  top: \"Tile1\"\n",
    "  tile_param {\n",
    "    axis: 1\n",
    "    tiles: 10\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"norm\"\n",
    "  type: \"Eltwise\"\n",
    "  bottom: \"feat\"\n",
    "  bottom: \"Tile1\"\n",
    "  top: \"embedding\"\n",
    "  eltwise_param {\n",
    "    operation: PROD\n",
    "  }\n",
    "}\n",
    "\n",
    "############# pairwise distance ###############\n",
    "layer {\n",
    "  name: \"pairwise_dist\"\n",
    "  type: \"Python\"\n",
    "  bottom: \"embedding\"\n",
    "  top: \"dist_matrix\"\n",
    "  python_param {\n",
    "    module: \"pairwise_distances_layer\"\n",
    "    layer: \"PairwiseDistancesLayer\"\n",
    "    param_str: '{\\\"margin\\\": 1.0}'\n",
    "  }\n",
    "}\n",
    "\n",
    "############# anc-pos/anc-neg mask ###############\n",
    "\n",
    "layer {\n",
    "  name: \"mask\"\n",
    "  type: \"Python\"\n",
    "  bottom: \"label\"\n",
    "  top: \"a_p_mask\"\n",
    "  top: \"a_n_mask\"\n",
    "  python_param {\n",
    "    module: \"triplet_mask_layer\"\n",
    "    layer: \"TripletMaskLayer\"\n",
    "  }\n",
    "}\n",
    "\n",
    "############# anc-pos dist ###############\n",
    "\n",
    "layer {\n",
    "  name: \"pos_filter\"\n",
    "  type: \"Eltwise\"\n",
    "  bottom: \"dist_matrix\"\n",
    "  bottom: \"a_p_mask\"\n",
    "  top: \"pos_survive\"\n",
    "  eltwise_param { operation: PROD }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"pos_argmax\"\n",
    "  type: \"ArgMax\"\n",
    "  bottom: \"pos_survive\"\n",
    "  top: \"pos_dist\"\n",
    "  argmax_param {\n",
    "  axis: 1\n",
    "  }\n",
    "}\n",
    "\n",
    "############# anc-neg dist ###############\n",
    "\n",
    "layer {\n",
    "  name: \"neg_filter\"\n",
    "  type: \"Eltwise\"\n",
    "  bottom: \"dist_matrix\"\n",
    "  bottom: \"a_n_mask\"\n",
    "  top: \"neg_survive\"\n",
    "  eltwise_param { operation: PROD }\n",
    "}\n",
    "\n",
    "\n",
    "layer {\n",
    "  name: \"scale1\"\n",
    "  bottom: \"neg_survive\"\n",
    "  top: \"neg_minus\"\n",
    "  type: \"Power\"\n",
    "  power_param {\n",
    "    scale: -1\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"neg_argmax\"\n",
    "  type: \"ArgMax\"\n",
    "  bottom: \"neg_minus\"\n",
    "  top: \"neg_argmax\"\n",
    "  argmax_param {\n",
    "  axis: 1\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"scale2\"\n",
    "  bottom: \"neg_argmax\"\n",
    "  top: \"neg_dist\"\n",
    "  type: \"Power\"\n",
    "  power_param {\n",
    "    scale: -1\n",
    "  }\n",
    "}\n",
    "\n",
    "############# loss ###############\n",
    "\n",
    "layer {\n",
    "  name: \"mean_triplet_loss\"\n",
    "  type: \"Python\"\n",
    "  bottom: \"pos_dist\"\n",
    "  bottom: \"neg_dist\"\n",
    "  top: \"loss\"\n",
    "  python_param {\n",
    "    module: \"mean_triplet_loss_layer\"\n",
    "    layer: \"MeanTripletLossLayer\"\n",
    "    param_str: '{\\\"debug\\\": 1.0}'\n",
    "  }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caffe_root = '../'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "sys.path.insert(0, caffe_root + 'examples/tripletloss')\n",
    "import caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import os\n",
    "mnist_data_dir = os.path.join(caffe_root, 'data/mnist')\n",
    "mndata = MNIST(mnist_data_dir)\n",
    "images, labels = mndata.load_training()\n",
    "print('loaded {} images, {} labels'.format(len(images), len(labels)))\n",
    "print('sample image at 0 = {}'.format(images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from StringIO import StringIO\n",
    "\n",
    "img_dir = '/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/images'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.makedirs(img_dir)\n",
    "\n",
    "# create a training list\n",
    "triplet_dict = {'anchor': None, 'positive': None, 'negative': None}\n",
    "anchor_list = StringIO()\n",
    "pos_list = StringIO()\n",
    "neg_list = StringIO()\n",
    "batch_str = StringIO()\n",
    "triplet_no = 0\n",
    "batch_size = 0\n",
    "for i, l in zip(images, labels):\n",
    "    array = np.array(i)\n",
    "    img = array.reshape((28, 28))\n",
    "    \n",
    "    if triplet_dict['anchor'] is None:\n",
    "        # this becomes an anchor\n",
    "        triplet_dict['anchor'] = [img, l]\n",
    "    elif triplet_dict['positive'] is None:\n",
    "        # check if this is the same label\n",
    "        if triplet_dict['anchor'][1] == l:\n",
    "            # this becomes a postive one\n",
    "            triplet_dict['positive'] = [img, l]\n",
    "    elif triplet_dict['anchor'][1] != l:\n",
    "        # this becomes a negative one\n",
    "        triplet_dict['negative'] = [img, l]\n",
    "        \n",
    "    if triplet_dict['negative'] is None:\n",
    "        continue\n",
    "    \n",
    "    # write\n",
    "    anchor_path = os.path.join(img_dir, '{}_anchor.jpg'.format(triplet_no))\n",
    "    pos_path = os.path.join(img_dir, '{}_positive.jpg'.format(triplet_no))\n",
    "    neg_path = os.path.join(img_dir, '{}_negative.jpg'.format(triplet_no))\n",
    "    \n",
    "    # image\n",
    "    cv2.imwrite(anchor_path, triplet_dict['anchor'][0])\n",
    "    cv2.imwrite(pos_path, triplet_dict['positive'][0])\n",
    "    cv2.imwrite(neg_path, triplet_dict['negative'][0])\n",
    "    \n",
    "    # sample\n",
    "    anchor_list.write('{} {}\\n'.format(anchor_path, triplet_dict['anchor'][1]))\n",
    "    pos_list.write('{} {}\\n'.format(pos_path, triplet_dict['positive'][1]))\n",
    "    neg_list.write('{} {}\\n'.format(neg_path, triplet_dict['negative'][1]))\n",
    "    \n",
    "    # reset\n",
    "    triplet_dict['anchor'] = None\n",
    "    triplet_dict['positive'] = None\n",
    "    triplet_dict['negative'] = None\n",
    "    \n",
    "    triplet_no += 1\n",
    "    batch_size += 1\n",
    "    \n",
    "    if batch_size == 64:\n",
    "        # write anchors first\n",
    "        batch_str.write(anchor_list.getvalue())\n",
    "        anchor_list.close()\n",
    "        anchor_list = StringIO()\n",
    "        # positive\n",
    "        batch_str.write(pos_list.getvalue())\n",
    "        pos_list.close()\n",
    "        pos_list = StringIO()\n",
    "        # negative\n",
    "        batch_str.write(neg_list.getvalue())\n",
    "        neg_list.close()\n",
    "        neg_list = StringIO()\n",
    "        # reset\n",
    "        batch_size = 0\n",
    "    \n",
    "# finally, write sample list\n",
    "with open(os.path.join(img_dir, '../' ,'trainlist_64.txt'), 'w') as f:\n",
    "    f.write(batch_str.getvalue())\n",
    "    batch_str.close()\n",
    "    anchor_list.close()\n",
    "    pos_list.close()\n",
    "    neg_list.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tripletloss/mnist_tripletloss_solver_10.prototxt\n",
    "# The train/test net protocol buffer definition\n",
    "train_net: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist_omoindrot_tripletloss_train_test_10.prototxt\"\n",
    "test_net: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist_omoindrot_tripletloss_train_test_10.prototxt\"\n",
    "# samples = 192 * 77 = 14784\n",
    "test_iter: 77\n",
    "# test at every epoch\n",
    "test_interval: 77\n",
    "# The base learning rate, momentum and the weight decay of the network.\n",
    "base_lr: 0.01\n",
    "momentum: 0.9\n",
    "weight_decay: 0.0005\n",
    "# The learning rate policy\n",
    "lr_policy: \"inv\"\n",
    "gamma: 0.0001\n",
    "power: 0.75\n",
    "# Display every epoch\n",
    "display: 77\n",
    "# The maximum number of iterations = 10 epochs\n",
    "max_iter: 770\n",
    "# snapshot intermediate results at every epoch\n",
    "snapshot: 77\n",
    "snapshot_prefix: \"/home/researcher/caffe-tripletloss/examples/tripletloss/mnist/mnist_tripletloss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "# reset solver to avoid a continuous training over multiple runs\n",
    "solver = None\n",
    "solver = caffe.SGDSolver('/home/researcher/caffe-tripletloss/examples/tripletloss/mnist_tripletloss_solver_10.prototxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each output is (batch size, feature dim, spatial dim)\n",
    "[(k, v.data.shape) for k, v in solver.net.blobs.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just print the weight sizes (we'll omit the biases)\n",
    "[(k, v[0].data.shape) for k, v in solver.net.params.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "itr_per_epoch = 77\n",
    "niter = itr_per_epoch * 20\n",
    "\n",
    "train_loss = np.zeros(niter)\n",
    "\n",
    "# the main solver loop\n",
    "for it in range(niter):\n",
    "    solver.step(1)  # SGD by Caffe\n",
    "    \n",
    "    # store the train loss\n",
    "    loss = solver.net.blobs['loss'].data\n",
    "    \n",
    "    # output every epoch\n",
    "    if it % itr_per_epoch == 0:\n",
    "        print('loss at epoch {} = {}'.format(it/itr_per_epoch, loss))\n",
    "    \n",
    "    train_loss[it] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenCV3.4.2+NVCaffeWithTriplet",
   "language": "python",
   "name": "py2_caffe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
